{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100_image_stitching_superpoint.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sksqcSZfOQx"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import scipy.io\n",
        "import os\n",
        "from numpy.linalg import norm\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy.linalg import det\n",
        "from numpy.linalg import inv\n",
        "from scipy.linalg import rq\n",
        "from numpy.linalg import svd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "from scipy import ndimage, spatial\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from skimage import io, transform,data\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sklearn.svm\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.cluster import completeness_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from functools import partial\n",
        "from torchsummary import summary\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "#cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "#accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#print(\"Accelerator type = \",accelerator)\n",
        "#print(\"Pytorch verision: \", torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm5uUA3ffbwl",
        "outputId": "dfc080fb-a9bd-46a5-df4c-261510fb141d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb_gZK3PfbtW",
        "outputId": "cb69187b-f136-46d1-9869-de51b7309223"
      },
      "source": [
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNiEuN2ZfbpQ"
      },
      "source": [
        "class Image:\n",
        "    def __init__(self, img, position):\n",
        "        \n",
        "        self.img = img\n",
        "        self.position = position\n",
        "\n",
        "inlier_matchset = []\n",
        "def features_matching(a,keypointlength,threshold):\n",
        "  #threshold=0.2\n",
        "  bestmatch=np.empty((keypointlength),dtype= np.int16)\n",
        "  img1index=np.empty((keypointlength),dtype=np.int16)\n",
        "  distance=np.empty((keypointlength))\n",
        "  index=0\n",
        "  for j in range(0,keypointlength):\n",
        "    #For a descriptor fa in Ia, take the two closest descriptors fb1 and fb2 in Ib\n",
        "    x=a[j]\n",
        "    listx=x.tolist()\n",
        "    x.sort()\n",
        "    minval1=x[0]                                # min \n",
        "    minval2=x[1]                                # 2nd min\n",
        "    itemindex1 = listx.index(minval1)           #index of min val    \n",
        "    itemindex2 = listx.index(minval2)           #index of second min value \n",
        "    ratio=minval1/minval2                       #Ratio Test\n",
        "    \n",
        "    if ratio<threshold: \n",
        "      #Low distance ratio: fb1 can be a good match\n",
        "      bestmatch[index]=itemindex1\n",
        "      distance[index]=minval1\n",
        "      img1index[index]=j\n",
        "      index=index+1\n",
        "  return  [cv2.DMatch(img1index[i],bestmatch[i].astype(int),distance[i]) for i in range(0,index)]\n",
        "          \n",
        "   \n",
        "  \n",
        "def compute_Homography(im1_pts,im2_pts):\n",
        "  \"\"\"\n",
        "  im1_pts and im2_pts are 2Ã—n matrices with\n",
        "  4 point correspondences from the two images\n",
        "  \"\"\"\n",
        "  num_matches=len(im1_pts)\n",
        "  num_rows = 2 * num_matches\n",
        "  num_cols = 9\n",
        "  A_matrix_shape = (num_rows,num_cols)\n",
        "  A = np.zeros(A_matrix_shape)\n",
        "  a_index = 0\n",
        "  for i in range(0,num_matches):\n",
        "    (a_x, a_y) = im1_pts[i]\n",
        "    (b_x, b_y) = im2_pts[i]\n",
        "    row1 = [a_x, a_y, 1, 0, 0, 0, -b_x*a_x, -b_x*a_y, -b_x] # First row \n",
        "    row2 = [0, 0, 0, a_x, a_y, 1, -b_y*a_x, -b_y*a_y, -b_y] # Second row \n",
        "\n",
        "    # place the rows in the matrix\n",
        "    A[a_index] = row1\n",
        "    A[a_index+1] = row2\n",
        "\n",
        "    a_index += 2\n",
        "    \n",
        "  U, s, Vt = np.linalg.svd(A)\n",
        "\n",
        "  #s is a 1-D array of singular values sorted in descending order\n",
        "  #U, Vt are unitary matrices\n",
        "  #Rows of Vt are the eigenvectors of A^TA.\n",
        "  #Columns of U are the eigenvectors of AA^T.\n",
        "  H = np.eye(3)\n",
        "  H = Vt[-1].reshape(3,3) # take the last row of the Vt matrix\n",
        "  return H\n",
        "  \n",
        "  \n",
        "def displayplot(img,title):\n",
        "  \n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.title(title)\n",
        "  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KtEsz46fbl0"
      },
      "source": [
        "def get_inliers(f1, f2, matches, H, RANSACthresh):\n",
        "\n",
        "  inlier_indices = []\n",
        "  for i in range(len(matches)):\n",
        "    queryInd = matches[i].queryIdx\n",
        "    trainInd = matches[i].trainIdx\n",
        "\n",
        "    #queryInd = matches[i][0]\n",
        "    #trainInd = matches[i][1]\n",
        "\n",
        "    queryPoint = np.array([f1[queryInd].pt[0],  f1[queryInd].pt[1], 1]).T \n",
        "    trans_query = H.dot(queryPoint) \n",
        "\n",
        "   \n",
        "    comp1 = [trans_query[0]/trans_query[2], trans_query[1]/trans_query[2]] # normalize with respect to z\n",
        "    comp2 = np.array(f2[trainInd].pt)[:2]\n",
        "    \n",
        "\n",
        "    if(np.linalg.norm(comp1-comp2) <= RANSACthresh): # check against threshold\n",
        "      inlier_indices.append(i)\n",
        "  return inlier_indices\n",
        "\n",
        "\n",
        "def RANSAC_alg(f1, f2, matches, nRANSAC, RANSACthresh):\n",
        "\n",
        "      \n",
        "    minMatches = 4\n",
        "    nBest = 0\n",
        "    best_inliers = []\n",
        "    H_estimate = np.eye(3,3)\n",
        "    global inlier_matchset\n",
        "    inlier_matchset=[]\n",
        "    for iteration in range(nRANSAC):\n",
        "      \n",
        "        #Choose a minimal set of feature matches.\n",
        "        matchSample = random.sample(matches, minMatches)\n",
        "        \n",
        "        #Estimate the Homography implied by these matches\n",
        "        im1_pts=np.empty((minMatches,2))\n",
        "        im2_pts=np.empty((minMatches,2))\n",
        "        for i in range(0,minMatches):\n",
        "          m = matchSample[i]\n",
        "          im1_pts[i] = f1[m.queryIdx].pt\n",
        "          im2_pts[i] = f2[m.trainIdx].pt\n",
        "          #im1_pts[i] = f1[m[0]].pt\n",
        "          #im2_pts[i] = f2[m[1]].pt             \n",
        "          \n",
        "        H_estimate=compute_Homography(im1_pts,im2_pts)\n",
        "        \n",
        "               \n",
        "        # Calculate the inliers for the H\n",
        "        inliers = get_inliers(f1, f2, matches, H_estimate, RANSACthresh)\n",
        "\n",
        "        # if the number of inliers is higher than previous iterations, update the best estimates\n",
        "        if len(inliers) > nBest:\n",
        "            nBest= len(inliers)\n",
        "            best_inliers = inliers\n",
        "\n",
        "    print(\"Number of best inliers\",len(best_inliers))\n",
        "    for i in range(len(best_inliers)):\n",
        "      inlier_matchset.append(matches[best_inliers[i]])\n",
        "    \n",
        "    # compute a homography given this set of matches\n",
        "    im1_pts=np.empty((len(best_inliers),2))\n",
        "    im2_pts=np.empty((len(best_inliers),2))\n",
        "    for i in range(0,len(best_inliers)):\n",
        "      m = inlier_matchset[i]\n",
        "      im1_pts[i] = f1[m.queryIdx].pt\n",
        "      im2_pts[i] = f2[m.trainIdx].pt\n",
        "      #im1_pts[i] = f1[m[0]].pt\n",
        "      #im2_pts[i] = f2[m[1]].pt\n",
        "\n",
        "    M=compute_Homography(im1_pts,im2_pts)\n",
        "    return M, best_inliers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PstvHQy9fbhf"
      },
      "source": [
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkU8gH_EfbdS"
      },
      "source": [
        "files_all=[]\n",
        "for file in os.listdir(\"/content/drive/MyDrive/RGB-img/img/\"):\n",
        "    if file.endswith(\".JPG\"):\n",
        "      files_all.append(file)\n",
        "\n",
        "\n",
        "files_all.sort()\n",
        "folder_path = '/content/drive/MyDrive/RGB-img/img/'\n",
        "\n",
        "centre_file = folder_path + files_all[50]\n",
        "left_files_path_rev = []\n",
        "right_files_path = []\n",
        "\n",
        "for file in files_all[:61]:\n",
        "  left_files_path_rev.append(folder_path + file)\n",
        "\n",
        "left_files_path = left_files_path_rev[::-1]\n",
        "\n",
        "for file in files_all[60:120]:\n",
        "  right_files_path.append(folder_path + file)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygOkDFKnfbY2",
        "outputId": "2188aa3b-9245-46a0-df51-a78b7592206f"
      },
      "source": [
        "gridsize = 8\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n",
        "\n",
        "images_left_bgr = []\n",
        "images_right_bgr = []\n",
        "\n",
        "images_left = []\n",
        "images_right = []\n",
        "\n",
        "for file in tqdm(left_files_path):\n",
        "  left_image_sat= cv2.imread(file)\n",
        "  #lab = cv2.cvtColor(left_image_sat, cv2.COLOR_BGR2LAB)\n",
        "  #lab[...,0] = clahe.apply(lab[...,0])\n",
        "  #left_image_sat = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  left_img = cv2.resize(left_image_sat,None,fx=0.3, fy=0.3, interpolation = cv2.INTER_CUBIC)\n",
        "  images_left.append(cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY).astype('float32')/255.)\n",
        "  images_left_bgr.append(left_img)\n",
        "\n",
        "\n",
        "for file in tqdm(right_files_path):\n",
        "  right_image_sat= cv2.imread(file)\n",
        "  #lab = cv2.cvtColor(right_image_sat, cv2.COLOR_BGR2LAB)\n",
        "  #lab[...,0] = clahe.apply(lab[...,0])\n",
        "  #right_image_sat = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  right_img = cv2.resize(right_image_sat,None,fx=0.3,fy=0.3, interpolation = cv2.INTER_CUBIC)\n",
        "  images_right.append(cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY).astype('float32')/255.)\n",
        "  images_right_bgr.append(right_img)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [01:12<00:00,  1.19s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:17<00:00,  1.28s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMXVUgLYfbVD",
        "outputId": "eba72804-888c-4f4a-80a4-c87953f4189e"
      },
      "source": [
        "images_left_bgr_no_enhance = []\n",
        "images_right_bgr_no_enhance = []\n",
        "\n",
        "for file in tqdm(left_files_path):\n",
        "  left_image_sat= cv2.imread(file)\n",
        "  left_img = cv2.resize(left_image_sat,None,fx=0.35, fy=0.35, interpolation = cv2.INTER_CUBIC)\n",
        "  images_left_bgr_no_enhance.append(left_img)\n",
        "\n",
        "\n",
        "for file in tqdm(right_files_path):\n",
        "  right_image_sat= cv2.imread(file)\n",
        "  right_img = cv2.resize(right_image_sat,None,fx=0.35,fy=0.35, interpolation = cv2.INTER_CUBIC)\n",
        "  images_right_bgr_no_enhance.append(right_img)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:28<00:00,  2.15it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:27<00:00,  2.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7uxhq-zfbQn",
        "outputId": "48672074-e665-4107-f533-800ae745dd34"
      },
      "source": [
        "!git clone https://github.com/magicleap/SuperPointPretrainedNetwork.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SuperPointPretrainedNetwork'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 81\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpS4UksvfbMm"
      },
      "source": [
        "weights_path = 'SuperPointPretrainedNetwork/superpoint_v1.pth'\n",
        "\n",
        "cuda = 'True'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG-8WcxefbIW"
      },
      "source": [
        "def to_kpts(pts, size=1):\n",
        "  return [cv2.KeyPoint(pt[0], pt[1], size) for pt in pts]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL457R0JfbDi"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class SuperPointNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SuperPointNet, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
        "        # Shared Encoder.\n",
        "        self.conv1a = nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1b = nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2a = nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2b = nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3a = nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3b = nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4a = nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4b = nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
        "        # Detector Head.\n",
        "        self.convPa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
        "        self.convPb = nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
        "        # Descriptor Head.\n",
        "        self.convDa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
        "        self.convDb = nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Shared Encoder.\n",
        "        x = self.relu(self.conv1a(x))\n",
        "        x = self.relu(self.conv1b(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2a(x))\n",
        "        x = self.relu(self.conv2b(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3a(x))\n",
        "        x = self.relu(self.conv3b(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv4a(x))\n",
        "        x = self.relu(self.conv4b(x))\n",
        "        # Detector Head.\n",
        "        cPa = self.relu(self.convPa(x))\n",
        "        semi = self.convPb(cPa)\n",
        "        # Descriptor Head.\n",
        "        cDa = self.relu(self.convDa(x))\n",
        "        desc = self.convDb(cDa)\n",
        "        dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
        "        desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
        "        return semi, desc\n",
        "\n",
        "\n",
        "class SuperPointFrontend(object):\n",
        "    def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,cuda=True):\n",
        "        self.name = 'SuperPoint'\n",
        "        self.cuda = cuda\n",
        "        self.nms_dist = nms_dist\n",
        "        self.conf_thresh = conf_thresh\n",
        "        self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
        "        self.cell = 8 # Size of each output cell. Keep this fixed.\n",
        "        self.border_remove = 4 # Remove points this close to the border.\n",
        "\n",
        "        # Load the network in inference mode.\n",
        "        self.net = SuperPointNet()\n",
        "        if cuda:\n",
        "          # Train on GPU, deploy on GPU.\n",
        "            self.net.load_state_dict(torch.load(weights_path))\n",
        "            self.net = self.net.cuda()\n",
        "        else:\n",
        "          # Train on GPU, deploy on CPU.\n",
        "            self.net.load_state_dict(torch.load(weights_path, map_location=lambda storage, loc: storage))\n",
        "        self.net.eval()\n",
        "\n",
        "    def nms_fast(self, in_corners, H, W, dist_thresh):\n",
        "\n",
        "        grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
        "        inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
        "        # Sort by confidence and round to nearest int.\n",
        "        inds1 = np.argsort(-in_corners[2,:])\n",
        "        corners = in_corners[:,inds1]\n",
        "        rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
        "        # Check for edge case of 0 or 1 corners.\n",
        "        if rcorners.shape[1] == 0:\n",
        "            return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
        "        if rcorners.shape[1] == 1:\n",
        "            out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
        "            return out, np.zeros((1)).astype(int)\n",
        "        # Initialize the grid.\n",
        "        for i, rc in enumerate(rcorners.T):\n",
        "            grid[rcorners[1,i], rcorners[0,i]] = 1\n",
        "            inds[rcorners[1,i], rcorners[0,i]] = i\n",
        "        # Pad the border of the grid, so that we can NMS points near the border.\n",
        "        pad = dist_thresh\n",
        "        grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
        "        # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
        "        count = 0\n",
        "        for i, rc in enumerate(rcorners.T):\n",
        "          # Account for top and left padding.\n",
        "            pt = (rc[0]+pad, rc[1]+pad)\n",
        "            if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
        "                grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
        "                grid[pt[1], pt[0]] = -1\n",
        "                count += 1\n",
        "        # Get all surviving -1's and return sorted array of remaining corners.\n",
        "        keepy, keepx = np.where(grid==-1)\n",
        "        keepy, keepx = keepy - pad, keepx - pad\n",
        "        inds_keep = inds[keepy, keepx]\n",
        "        out = corners[:, inds_keep]\n",
        "        values = out[-1, :]\n",
        "        inds2 = np.argsort(-values)\n",
        "        out = out[:, inds2]\n",
        "        out_inds = inds1[inds_keep[inds2]]\n",
        "        return out, out_inds\n",
        "\n",
        "    def run(self, img):\n",
        "        assert img.ndim == 2 #Image must be grayscale.\n",
        "        assert img.dtype == np.float32 #Image must be float32.\n",
        "        H, W = img.shape[0], img.shape[1]\n",
        "        inp = img.copy()\n",
        "        inp = (inp.reshape(1, H, W))\n",
        "        inp = torch.from_numpy(inp)\n",
        "        inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
        "        if self.cuda:\n",
        "            inp = inp.cuda()\n",
        "        # Forward pass of network.\n",
        "        outs = self.net.forward(inp)\n",
        "        semi, coarse_desc = outs[0], outs[1]\n",
        "        # Convert pytorch -> numpy.\n",
        "        semi = semi.data.cpu().numpy().squeeze()\n",
        "        \n",
        "        # --- Process points.\n",
        "        dense = np.exp(semi) # Softmax.\n",
        "        dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
        "        nodust = dense[:-1, :, :]\n",
        "        # Reshape to get full resolution heatmap.\n",
        "        Hc = int(H / self.cell)\n",
        "        Wc = int(W / self.cell)\n",
        "        nodust = np.transpose(nodust, [1, 2, 0])\n",
        "        heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
        "        heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
        "        heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell]) \n",
        "        prob_map = heatmap/np.sum(np.sum(heatmap))\n",
        "        \n",
        "        return heatmap, coarse_desc\n",
        "\n",
        "\n",
        "    def key_pt_sampling(self, img, heat_map, coarse_desc, sampled):\n",
        "        \n",
        "        H, W = img.shape[0], img.shape[1]\n",
        "\n",
        "        xs, ys = np.where(heat_map >= self.conf_thresh) # Confidence threshold.\n",
        "        if len(xs) == 0:\n",
        "            return np.zeros((3, 0)), None, None\n",
        "        print(\"number of pts selected :\", len(xs))\n",
        "        \n",
        "        \n",
        "        pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
        "        pts[0, :] = ys\n",
        "        pts[1, :] = xs\n",
        "        pts[2, :] = heat_map[xs, ys]\n",
        "        pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
        "        inds = np.argsort(pts[2,:])\n",
        "        pts = pts[:,inds[::-1]] # Sort by confidence.\n",
        "        bord = self.border_remove\n",
        "        toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
        "        toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
        "        toremove = np.logical_or(toremoveW, toremoveH)\n",
        "        pts = pts[:, ~toremove]\n",
        "        pts = pts[:,0:sampled] #we take 2000 keypoints with highest probability from heatmap for our benchmark\n",
        "        \n",
        "        # --- Process descriptor.\n",
        "        D = coarse_desc.shape[1]\n",
        "        if pts.shape[1] == 0:\n",
        "            desc = np.zeros((D, 0))\n",
        "        else:\n",
        "          # Interpolate into descriptor map using 2D point locations.\n",
        "            samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
        "            samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
        "            samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
        "            samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
        "            samp_pts = samp_pts.view(1, 1, -1, 2)\n",
        "            samp_pts = samp_pts.float()\n",
        "            if self.cuda:\n",
        "                samp_pts = samp_pts.cuda()            \n",
        "            desc = nn.functional.grid_sample(coarse_desc, samp_pts)\n",
        "            desc = desc.data.cpu().numpy().reshape(D, -1)\n",
        "            desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
        "\n",
        "            \n",
        "        return pts, desc"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "Zvms7Ozgfa-1",
        "outputId": "0de4f36c-25a1-40d1-e328-234714fe3b6c"
      },
      "source": [
        "print('Loading pre-trained network.')\n",
        "# This class runs the SuperPoint network and processes its outputs.\n",
        "fe = SuperPointFrontend(weights_path=weights_path,nms_dist = 3,conf_thresh = 0.01,nn_thresh=0.5)\n",
        "print('Successfully loaded pre-trained network.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pre-trained network.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-30114529fc6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading pre-trained network.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This class runs the SuperPoint network and processes its outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSuperPointFrontend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnms_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconf_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Successfully loaded pre-trained network.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-cdae3825bc93>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights_path, nms_dist, conf_thresh, nn_thresh, cuda)\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0;31m# Train on GPU, deploy on GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0;31m# Train on GPU, deploy on CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73FiB2X6fa5l"
      },
      "source": [
        "keypoints_all_left_superpoint = []\n",
        "descriptors_all_left_superpoint = []\n",
        "points_all_left_superpoint=[]\n",
        "\n",
        "keypoints_all_right_superpoint = []\n",
        "descriptors_all_right_superpoint = []\n",
        "points_all_right_superpoint=[]\n",
        "\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "for lfpth in tqdm(images_left):\n",
        "  heatmap1, coarse_desc1 = fe.run(lfpth)\n",
        "  pts_1, desc_1 = fe.key_pt_sampling(lfpth, heatmap1, coarse_desc1, 80000) #Getting keypoints and descriptors for 1st image\n",
        "\n",
        "  keypoints_all_left_superpoint.append(to_kpts(pts_1.T))\n",
        "  descriptors_all_left_superpoint.append(desc_1.T)\n",
        "  points_all_left_superpoint.append(pts_1.T)\n",
        "\n",
        "\n",
        "for rfpth in tqdm(images_right):\n",
        "  heatmap1, coarse_desc1 = fe.run(rfpth)\n",
        "  pts_1, desc_1 = fe.key_pt_sampling(rfpth, heatmap1, coarse_desc1, 80000) #Getting keypoints and descriptors for 1st image\n",
        "\n",
        "  keypoints_all_right_superpoint.append(to_kpts(pts_1.T))\n",
        "  descriptors_all_right_superpoint.append(desc_1.T)\n",
        "  points_all_right_superpoint.append(pts_1.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpcYq3jafa07"
      },
      "source": [
        "num_kps_superpoint = []\n",
        "for j in tqdm(keypoints_all_left_superpoint + keypoints_all_right_superpoint):\n",
        "  num_kps_superpoint.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg_UlyBofave"
      },
      "source": [
        "def compute_homography_fast(matched_pts1, matched_pts2,thresh=4):\n",
        "    #matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
        "    #matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
        "\n",
        "    # Estimate the homography between the matches using RANSAC\n",
        "    H, inliers = cv2.findHomography(matched_pts1,\n",
        "                                    matched_pts2,\n",
        "                                    cv2.RANSAC, ransacReprojThreshold =thresh)\n",
        "    inliers = inliers.flatten()\n",
        "    return H, inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq8APYCVgcIp"
      },
      "source": [
        "def compute_homography_fast_other(matched_pts1, matched_pts2):\n",
        "    #matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
        "    #matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
        "\n",
        "    # Estimate the homography between the matches using RANSAC\n",
        "    H, inliers = cv2.findHomography(matched_pts1,\n",
        "                                    matched_pts2,\n",
        "                                    0)\n",
        "    inliers = inliers.flatten()\n",
        "    return H, inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifA1y9igcFN"
      },
      "source": [
        "def get_Hmatrix(imgs,keypts,pts,descripts,ratio=0.8,thresh=4,use_lowe=True,disp=False,no_ransac=False,binary=False):\n",
        "  lff1 = descripts[0]\n",
        "  lff = descripts[1]\n",
        "\n",
        "  if use_lowe==False:\n",
        "    #FLANN_INDEX_KDTREE = 2\n",
        "    #index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    #search_params = dict(checks=50)\n",
        "    #flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    #flann = cv2.BFMatcher()\n",
        "    if binary==True:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "    else:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "\n",
        "\n",
        "    #matches_lf1_lf = flann.knnMatch(lff1, lff, k=2)\n",
        "    matches_4 = bf.match(lff1, lff)\n",
        "    matches_lf1_lf = []\n",
        "\n",
        "\n",
        "    print(\"\\nNumber of matches\",len(matches_4))\n",
        "    '''\n",
        "    matches_4 = []\n",
        "    ratio = ratio\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Loweâ€™s ratio test)\n",
        "      #if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "      matches_4.append(m[0])\n",
        "    '''\n",
        "    print(\"Number of matches After Lowe's Ratio\",len(matches_4))\n",
        "  else:\n",
        "    FLANN_INDEX_KDTREE = 2\n",
        "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    search_params = dict(checks=50)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    if binary==True:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "    else:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "\n",
        "\n",
        "    matches_lf1_lf = flann.knnMatch(lff1, lff, k=2)\n",
        "    #matches_lf1_lf = bf.knnMatch(lff1, lff,k=2)\n",
        "\n",
        "\n",
        "    print(\"\\nNumber of matches\",len(matches_lf1_lf))\n",
        "    matches_4 = []\n",
        "    ratio = ratio\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Loweâ€™s ratio test)\n",
        "      if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "        matches_4.append(m[0])\n",
        "  \n",
        "    print(\"Number of matches After Lowe's Ratio\",len(matches_4))\n",
        "\n",
        "\n",
        "  \n",
        "  matches_idx = np.array([m.queryIdx for m in matches_4])\n",
        "  imm1_pts = np.array([keypts[0][idx].pt for idx in matches_idx])\n",
        "  matches_idx = np.array([m.trainIdx for m in matches_4])\n",
        "  imm2_pts = np.array([keypts[1][idx].pt for idx in matches_idx])\n",
        "  '''\n",
        "  # Estimate homography 1\n",
        "  #Compute H1\n",
        "  # Estimate homography 1\n",
        "  #Compute H1\n",
        "  imm1_pts=np.empty((len(matches_4),2))\n",
        "  imm2_pts=np.empty((len(matches_4),2))\n",
        "  for i in range(0,len(matches_4)):\n",
        "    m = matches_4[i]\n",
        "    (a_x, a_y) = keypts[0][m.queryIdx].pt\n",
        "    (b_x, b_y) = keypts[1][m.trainIdx].pt\n",
        "    imm1_pts[i]=(a_x, a_y)\n",
        "    imm2_pts[i]=(b_x, b_y)    \n",
        "  H=compute_Homography(imm1_pts,imm2_pts) \n",
        "  #Robustly estimate Homography 1 using RANSAC\n",
        "  Hn, best_inliers=RANSAC_alg(keypts[0] ,keypts[1], matches_4,  nRANSAC=1000, RANSACthresh=6)\n",
        "  '''\n",
        "  \n",
        "  if no_ransac==True:\n",
        "    Hn,inliers = compute_homography_fast_other(imm1_pts,imm2_pts)\n",
        "  else:\n",
        "    Hn,inliers = compute_homography_fast(imm1_pts,imm2_pts,thresh)  \n",
        "\n",
        "  inlier_matchset = np.array(matches_4)[inliers.astype(bool)].tolist()\n",
        "  print(\"Number of Robust matches\",len(inlier_matchset))\n",
        "  print(\"\\n\")\n",
        "  '''\n",
        "  if len(inlier_matchset)<50:\n",
        "    matches_4 = []\n",
        "    ratio = 0.67\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Loweâ€™s ratio test)\n",
        "      if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "          matches_4.append(m[0])\n",
        "    print(\"Number of matches After Lowe's Ratio New\",len(matches_4))\n",
        "  \n",
        "    matches_idx = np.array([m.queryIdx for m in matches_4])\n",
        "    imm1_pts = np.array([keypts[0][idx].pt for idx in matches_idx])\n",
        "    matches_idx = np.array([m.trainIdx for m in matches_4])\n",
        "    imm2_pts = np.array([keypts[1][idx].pt for idx in matches_idx])\n",
        "    Hn,inliers = compute_homography_fast_other(imm1_pts,imm2_pts)  \n",
        "    inlier_matchset = np.array(matches_4)[inliers.astype(bool)].tolist()\n",
        "    print(\"Number of Robust matches New\",len(inlier_matchset))\n",
        "    print(\"\\n\")    \n",
        "    '''\n",
        "  #H=compute_Homography(imm1_pts,imm2_pts) \n",
        "  #Robustly estimate Homography 1 using RANSAC\n",
        "  #Hn=RANSAC_alg(keypts[0] ,keypts[1], matches_4,  nRANSAC=1500, RANSACthresh=6)\n",
        "\n",
        "  #global inlier_matchset   \n",
        "  \n",
        "  if disp==True:\n",
        "    dispimg1=cv2.drawMatches(imgs[0], keypts[0], imgs[1], keypts[1], inlier_matchset, None,flags=2)\n",
        "    displayplot(dispimg1,'Robust Matching between Reference Image and Right Image ')\n",
        "  \n",
        "  return Hn/Hn[2,2], len(matches_lf1_lf), len(inlier_matchset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfsrHBRxgcBk"
      },
      "source": [
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmI_gAasgb-L"
      },
      "source": [
        "print(left_files_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdZgshPsg5Wl"
      },
      "source": [
        "print(right_files_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xvUow5Fg5Rm"
      },
      "source": [
        "H_left_superpoint = []\n",
        "H_right_superpoint = []\n",
        "\n",
        "num_matches_superpoint = []\n",
        "num_good_matches_superpoint = []\n",
        "\n",
        "for j in tqdm(range(len(images_left))):\n",
        "  if j==len(images_left)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_superpoint[j:j+2][::-1],points_all_left_superpoint[j:j+2][::-1],descriptors_all_left_superpoint[j:j+2][::-1],ratio=0.75,thresh=10,no_ransac=False)\n",
        "  H_left_superpoint.append(H_a)\n",
        "  num_matches_superpoint.append(matches)\n",
        "  num_good_matches_superpoint.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(images_right))):\n",
        "  if j==len(images_right)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_superpoint[j:j+2][::-1],points_all_right_superpoint[j:j+2][::-1],descriptors_all_right_superpoint[j:j+2][::-1],ratio=0.9,thresh = 10,no_ransac=False)\n",
        "  H_right_superpoint.append(H_a)\n",
        "  num_matches_superpoint.append(matches)\n",
        "  num_good_matches_superpoint.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQhwMtZNg5Ml"
      },
      "source": [
        "def warpnImages(images_left, images_right,H_left,H_right):\n",
        "    #img1-centre,img2-left,img3-right\n",
        "\n",
        "    h, w = images_left[0].shape[:2]\n",
        "\n",
        "    pts_left = []\n",
        "    pts_right = []\n",
        "\n",
        "    pts_centre = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "    for j in range(len(H_left)):\n",
        "      pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
        "      pts_left.append(pts)\n",
        "\n",
        "    for j in range(len(H_right)):\n",
        "      pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
        "      pts_right.append(pts)\n",
        "\n",
        "    pts_left_transformed=[]\n",
        "    pts_right_transformed=[]\n",
        "\n",
        "\n",
        "\n",
        "    for j,pts in enumerate(pts_left):\n",
        "      if j==0:\n",
        "        H_trans = H_left[j]\n",
        "      else:\n",
        "        H_trans = H_trans@H_left[j]\n",
        "      pts_ = cv2.perspectiveTransform(pts, H_trans)\n",
        "      pts_left_transformed.append(pts_)\n",
        "\n",
        "    for j,pts in enumerate(pts_right):\n",
        "      if j==0:\n",
        "        H_trans = H_right[j]\n",
        "      else:\n",
        "        H_trans = H_trans@H_right[j]\n",
        "      pts_ = cv2.perspectiveTransform(pts, H_trans)\n",
        "      pts_right_transformed.append(pts_)\n",
        "\n",
        "\n",
        "    print('Step1:Done')\n",
        "\n",
        "\n",
        "    #pts = np.concatenate((pts1, pts2_), axis=0)\n",
        "\n",
        "    pts_concat = np.concatenate((pts_centre,np.concatenate(np.array(pts_left_transformed),axis=0),np.concatenate(np.array(pts_right_transformed),axis=0)), axis=0)\n",
        "\n",
        "    [xmin, ymin] = np.int32(pts_concat.min(axis=0).ravel() - 0.5)\n",
        "    [xmax, ymax] = np.int32(pts_concat.max(axis=0).ravel() + 0.5)\n",
        "    t = [-xmin, -ymin]\n",
        "    Ht = np.array([[1, 0, t[0]], [0, 1, t[1]], [0, 0, 1]])  # translate\n",
        "\n",
        "    print('Step2:Done')\n",
        "\n",
        "\n",
        "    return xmax,xmin,ymax,ymin,t,h,w,Ht"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33XOSpDzg5Ge"
      },
      "source": [
        "def final_steps_left(images_left,images_right,H_left,H_right,xmax,xmin,ymax,ymin,t,h,w,Ht):\n",
        "\n",
        "    warp_imgs_left = []\n",
        "\n",
        "\n",
        "    for j,H in enumerate(H_left):\n",
        "      if j==0:\n",
        "        H_trans = Ht@H\n",
        "      else:\n",
        "        H_trans = H_trans@H\n",
        "      result = cv2.warpPerspective(images_left[j+1], H_trans, (xmax-xmin, ymax-ymin))\n",
        "\n",
        "      if j==0:\n",
        "        result[t[1]:h+t[1], t[0]:w+t[0]] = images_left[0]\n",
        "      \n",
        "      warp_imgs_left.append(result)\n",
        "\n",
        "    print('Step31:Done')\n",
        "\n",
        "    return warp_imgs_left\n",
        "\n",
        "def final_steps_right(images_left,images_right,H_left,H_right,xmax,xmin,ymax,ymin,t,h,w,Ht):\n",
        "\n",
        "    warp_imgs_right = []\n",
        "\n",
        "    for j,H in enumerate(H_right):\n",
        "      if j==0:\n",
        "        H_trans = Ht@H\n",
        "      else:\n",
        "        H_trans = H_trans@H\n",
        "      result = cv2.warpPerspective(images_right[j+1], H_trans, (xmax-xmin, ymax-ymin))\n",
        "      \n",
        "      warp_imgs_right.append(result)\n",
        "\n",
        "    print('Step32:Done')\n",
        "\n",
        "    return warp_imgs_right\n",
        "\n",
        "def final_steps_union(warp_imgs_left,warp_imgs_right):\n",
        "    #Union\n",
        "\n",
        "    warp_images_all = warp_imgs_left + warp_imgs_right\n",
        "\n",
        "    warp_img_init = warp_images_all[0]\n",
        "\n",
        "\n",
        "\n",
        "    #warp_final_all=[]\n",
        "\n",
        "    for j,warp_img in enumerate(warp_images_all):\n",
        "      if j==len(warp_images_all)-1:\n",
        "        break\n",
        "      black_pixels = np.where((warp_img_init[:, :, 0] == 0) & (warp_img_init[:, :, 1] == 0) & (warp_img_init[:, :, 2] == 0))\n",
        "\n",
        "      warp_img_init[black_pixels] = warp_images_all[j+1][black_pixels]\n",
        "    \n",
        "      #warp_final = np.maximum(warp_img_init,warp_images_all[j+1])\n",
        "      #warp_img_init = warp_final\n",
        "      #warp_final_all.append(warp_final)\n",
        "\n",
        "    print('Step4:Done')\n",
        "\n",
        "\n",
        "    return warp_img_init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8M926mng492"
      },
      "source": [
        "def final_steps_left_union(images_left,H_left,xmax,xmin,ymax,ymin,t,h,w,Ht):\n",
        "\n",
        "\n",
        "\n",
        "    for j,H in enumerate(H_left):\n",
        "      if j==0:\n",
        "        H_trans = Ht@H\n",
        "      else:\n",
        "        H_trans = H_trans@H\n",
        "      result = cv2.warpPerspective(images_left[j+1], H_trans, (xmax-xmin, ymax-ymin))\n",
        "      warp_img_init_curr = result\n",
        "\n",
        "      if j==0:\n",
        "        result[t[1]:h+t[1], t[0]:w+t[0]] = images_left[0]\n",
        "        warp_img_init_prev = result\n",
        "        continue\n",
        "\n",
        "      black_pixels = np.where((warp_img_init_prev[:, :, 0] == 0) & (warp_img_init_prev[:, :, 1] == 0) & (warp_img_init_prev[:, :, 2] == 0))\n",
        "\n",
        "      warp_img_init_prev[black_pixels] = warp_img_init_curr[black_pixels]\n",
        "\n",
        "    print('Step31:Done')\n",
        "\n",
        "    return warp_img_init_prev\n",
        "\n",
        "def final_steps_right_union(warp_img_prev,images_right,H_right,xmax,xmin,ymax,ymin,t,h,w,Ht):\n",
        "\n",
        "    for j,H in enumerate(H_right):\n",
        "      if j==0:\n",
        "        H_trans = Ht@H\n",
        "      else:\n",
        "        H_trans = H_trans@H\n",
        "      result = cv2.warpPerspective(images_right[j+1], H_trans, (xmax-xmin, ymax-ymin))\n",
        "      warp_img_init_curr = result\n",
        "\n",
        "      black_pixels = np.where((warp_img_prev[:, :, 0] == 0) & (warp_img_prev[:, :, 1] == 0) & (warp_img_prev[:, :, 2] == 0))\n",
        "\n",
        "      warp_img_prev[black_pixels] = warp_img_init_curr[black_pixels]\n",
        "      \n",
        "    print('Step32:Done')\n",
        "\n",
        "    return warp_img_prev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjWI00r8hPvZ"
      },
      "source": [
        "print(left_files_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICGzfHAPhPpf"
      },
      "source": [
        "print(right_files_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZdVL44GhPjf"
      },
      "source": [
        "xmax,xmin,ymax,ymin,t,h,w,Ht = warpnImages(images_left_bgr, images_right_bgr,H_left_superpoint,H_right_brisk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alkg3THUhPdV"
      },
      "source": [
        "warp_imgs_left = final_steps_left_union(images_left_bgr,H_left_superpoint,xmax,xmin,ymax,ymin,t,h,w,Ht)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRCHxynPhnIS"
      },
      "source": [
        "warp_imgs_all_superpoint = final_steps_right_union(warp_imgs_left, images_right_bgr,H_right_superpoint,xmax,xmin,ymax,ymin,t,h,w,Ht)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F8f9yG8hnBJ"
      },
      "source": [
        "fig,ax =plt.subplots()\n",
        "fig.set_size_inches(20,20)\n",
        "ax.imshow(cv2.cvtColor(warp_imgs_all_superpoint , cv2.COLOR_BGR2RGB))\n",
        "ax.set_title('61-Images Mosaic-SUPERPOINT')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}