{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Benchmarking_(MAP)_with_Surf+Sift_detector_using_100_images(0.35x).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa8bfJcWCWFb"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import scipy.io\n",
        "import os\n",
        "from numpy.linalg import norm\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy.linalg import det\n",
        "from numpy.linalg import inv\n",
        "from scipy.linalg import rq\n",
        "from numpy.linalg import svd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "from scipy import ndimage, spatial\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from skimage import io, transform,data\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sklearn.svm\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.cluster import completeness_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from functools import partial\n",
        "from torchsummary import summary\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import h5py as h5\n",
        "\n",
        "#cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "#accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#print(\"Accelerator type = \",accelerator)\n",
        "#print(\"Pytorch verision: \", torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL9rcKHAChcb",
        "outputId": "769cb934-a03b-416a-839d-2676020edb65"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdSkrM1pGGNf"
      },
      "source": [
        "#!pip install ipython-autotime\n",
        "\n",
        "#%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VFwhM5HChUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95ede0c-018b-4e64-88a5-14d41db05ec6"
      },
      "source": [
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl3jpVdOChL1"
      },
      "source": [
        "class Image:\n",
        "    def __init__(self, img, position):\n",
        "        \n",
        "        self.img = img\n",
        "        self.position = position\n",
        "\n",
        "inlier_matchset = []\n",
        "def features_matching(a,keypointlength,threshold):\n",
        "  #threshold=0.2\n",
        "  bestmatch=np.empty((keypointlength),dtype= np.int16)\n",
        "  img1index=np.empty((keypointlength),dtype=np.int16)\n",
        "  distance=np.empty((keypointlength))\n",
        "  index=0\n",
        "  for j in range(0,keypointlength):\n",
        "    #For a descriptor fa in Ia, take the two closest descriptors fb1 and fb2 in Ib\n",
        "    x=a[j]\n",
        "    listx=x.tolist()\n",
        "    x.sort()\n",
        "    minval1=x[0]                                # min \n",
        "    minval2=x[1]                                # 2nd min\n",
        "    itemindex1 = listx.index(minval1)           #index of min val    \n",
        "    itemindex2 = listx.index(minval2)           #index of second min value \n",
        "    ratio=minval1/minval2                       #Ratio Test\n",
        "    \n",
        "    if ratio<threshold: \n",
        "      #Low distance ratio: fb1 can be a good match\n",
        "      bestmatch[index]=itemindex1\n",
        "      distance[index]=minval1\n",
        "      img1index[index]=j\n",
        "      index=index+1\n",
        "  return  [cv2.DMatch(img1index[i],bestmatch[i].astype(int),distance[i]) for i in range(0,index)]\n",
        "          \n",
        "   \n",
        "  \n",
        "def compute_Homography(im1_pts,im2_pts):\n",
        "  \"\"\"\n",
        "  im1_pts and im2_pts are 2×n matrices with\n",
        "  4 point correspondences from the two images\n",
        "  \"\"\"\n",
        "  num_matches=len(im1_pts)\n",
        "  num_rows = 2 * num_matches\n",
        "  num_cols = 9\n",
        "  A_matrix_shape = (num_rows,num_cols)\n",
        "  A = np.zeros(A_matrix_shape)\n",
        "  a_index = 0\n",
        "  for i in range(0,num_matches):\n",
        "    (a_x, a_y) = im1_pts[i]\n",
        "    (b_x, b_y) = im2_pts[i]\n",
        "    row1 = [a_x, a_y, 1, 0, 0, 0, -b_x*a_x, -b_x*a_y, -b_x] # First row \n",
        "    row2 = [0, 0, 0, a_x, a_y, 1, -b_y*a_x, -b_y*a_y, -b_y] # Second row \n",
        "\n",
        "    # place the rows in the matrix\n",
        "    A[a_index] = row1\n",
        "    A[a_index+1] = row2\n",
        "\n",
        "    a_index += 2\n",
        "    \n",
        "  U, s, Vt = np.linalg.svd(A)\n",
        "\n",
        "  #s is a 1-D array of singular values sorted in descending order\n",
        "  #U, Vt are unitary matrices\n",
        "  #Rows of Vt are the eigenvectors of A^TA.\n",
        "  #Columns of U are the eigenvectors of AA^T.\n",
        "  H = np.eye(3)\n",
        "  H = Vt[-1].reshape(3,3) # take the last row of the Vt matrix\n",
        "  return H\n",
        "  \n",
        "  \n",
        "def displayplot(img,title):\n",
        "  \n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.title(title)\n",
        "  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJz6bnF_CgaL"
      },
      "source": [
        "def get_inliers(f1, f2, matches, H, RANSACthresh):\n",
        "\n",
        "  inlier_indices = []\n",
        "  for i in range(len(matches)):\n",
        "    queryInd = matches[i].queryIdx\n",
        "    trainInd = matches[i].trainIdx\n",
        "\n",
        "    #queryInd = matches[i][0]\n",
        "    #trainInd = matches[i][1]\n",
        "\n",
        "    queryPoint = np.array([f1[queryInd].pt[0],  f1[queryInd].pt[1], 1]).T \n",
        "    trans_query = H.dot(queryPoint) \n",
        "\n",
        "   \n",
        "    comp1 = [trans_query[0]/trans_query[2], trans_query[1]/trans_query[2]] # normalize with respect to z\n",
        "    comp2 = np.array(f2[trainInd].pt)[:2]\n",
        "    \n",
        "\n",
        "    if(np.linalg.norm(comp1-comp2) <= RANSACthresh): # check against threshold\n",
        "      inlier_indices.append(i)\n",
        "  return inlier_indices\n",
        "\n",
        "\n",
        "def RANSAC_alg(f1, f2, matches, nRANSAC, RANSACthresh):\n",
        "\n",
        "      \n",
        "    minMatches = 4\n",
        "    nBest = 0\n",
        "    best_inliers = []\n",
        "    H_estimate = np.eye(3,3)\n",
        "    global inlier_matchset\n",
        "    inlier_matchset=[]\n",
        "    for iteration in range(nRANSAC):\n",
        "      \n",
        "        #Choose a minimal set of feature matches.\n",
        "        matchSample = random.sample(matches, minMatches)\n",
        "        \n",
        "        #Estimate the Homography implied by these matches\n",
        "        im1_pts=np.empty((minMatches,2))\n",
        "        im2_pts=np.empty((minMatches,2))\n",
        "        for i in range(0,minMatches):\n",
        "          m = matchSample[i]\n",
        "          im1_pts[i] = f1[m.queryIdx].pt\n",
        "          im2_pts[i] = f2[m.trainIdx].pt\n",
        "          #im1_pts[i] = f1[m[0]].pt\n",
        "          #im2_pts[i] = f2[m[1]].pt             \n",
        "          \n",
        "        H_estimate=compute_Homography(im1_pts,im2_pts)\n",
        "        \n",
        "               \n",
        "        # Calculate the inliers for the H\n",
        "        inliers = get_inliers(f1, f2, matches, H_estimate, RANSACthresh)\n",
        "\n",
        "        # if the number of inliers is higher than previous iterations, update the best estimates\n",
        "        if len(inliers) > nBest:\n",
        "            nBest= len(inliers)\n",
        "            best_inliers = inliers\n",
        "\n",
        "    print(\"Number of best inliers\",len(best_inliers))\n",
        "    for i in range(len(best_inliers)):\n",
        "      inlier_matchset.append(matches[best_inliers[i]])\n",
        "    \n",
        "    # compute a homography given this set of matches\n",
        "    im1_pts=np.empty((len(best_inliers),2))\n",
        "    im2_pts=np.empty((len(best_inliers),2))\n",
        "    for i in range(0,len(best_inliers)):\n",
        "      m = inlier_matchset[i]\n",
        "      im1_pts[i] = f1[m.queryIdx].pt\n",
        "      im2_pts[i] = f2[m.trainIdx].pt\n",
        "      #im1_pts[i] = f1[m[0]].pt\n",
        "      #im2_pts[i] = f2[m[1]].pt\n",
        "\n",
        "    M=compute_Homography(im1_pts,im2_pts)\n",
        "    return M, best_inliers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-a3Gc2CgHC"
      },
      "source": [
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ZzIO2rDcxW"
      },
      "source": [
        "files_all=[]\n",
        "for file in os.listdir(\"/content/drive/MyDrive/MAP-20210707T092105Z-001/MAP\"):\n",
        "    if file.endswith(\".JPG\"):\n",
        "      files_all.append(file)\n",
        "\n",
        "\n",
        "files_all.sort()\n",
        "folder_path = '/content/drive/MyDrive/MAP-20210707T092105Z-001/MAP/'\n",
        "\n",
        "#centre_file = folder_path + files_all[50]\n",
        "left_files_path_rev = []\n",
        "right_files_path = []\n",
        "\n",
        "\n",
        "#Change this according to your dataset split\n",
        "\n",
        "for file in files_all[:26]:\n",
        "  left_files_path_rev.append(folder_path + file)\n",
        "\n",
        "left_files_path = left_files_path_rev[::-1]\n",
        "\n",
        "for file in files_all[25:50]:\n",
        "  right_files_path.append(folder_path + file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqngRpI6UabS",
        "outputId": "102f8993-0563-4364-dc05-f1afd4c5b819"
      },
      "source": [
        "print(len(files_all))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERrE4NuoDcqr"
      },
      "source": [
        "from multiprocessing import Pool"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJYWbq9HDcaS",
        "outputId": "8c9bdec8-a949-4d39-9191-51005452f6a2"
      },
      "source": [
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiG8CVOtHaP-",
        "outputId": "a038da31-2a1b-4ddc-8131-b42ecde70fd5"
      },
      "source": [
        "gridsize = 8\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n",
        "\n",
        "images_left_bgr = []\n",
        "images_right_bgr = []\n",
        "\n",
        "images_left = []\n",
        "images_right = []\n",
        "\n",
        "for file in tqdm(left_files_path):\n",
        "  left_image_sat= cv2.imread(file)\n",
        "  #lab = cv2.cvtColor(left_image_sat, cv2.COLOR_BGR2LAB)\n",
        "  #lab[...,0] = clahe.apply(lab[...,0])\n",
        "  #left_image_sat = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  left_img = cv2.resize(left_image_sat,None,fx=0.75, fy=0.75, interpolation = cv2.INTER_CUBIC )\n",
        "  images_left.append(cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY).astype('float32')/255.)\n",
        "  images_left_bgr.append(left_img)\n",
        "\n",
        "\n",
        "for file in tqdm(right_files_path):\n",
        "  right_image_sat= cv2.imread(file)\n",
        "  #lab = cv2.cvtColor(right_image_sat, cv2.COLOR_BGR2LAB)\n",
        "  #lab[...,0] = clahe.apply(lab[...,0])\n",
        "  #right_image_sat = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  right_img = cv2.resize(right_image_sat,None,fx=0.75,fy=0.75, interpolation = cv2.INTER_CUBIC )\n",
        "  images_right.append(cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY).astype('float32')/255.)\n",
        "  images_right_bgr.append(right_img)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26/26 [00:13<00:00,  1.86it/s]\n",
            "100%|██████████| 25/25 [00:14<00:00,  1.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZZ5LCMiUjBR"
      },
      "source": [
        "Dataset = 'MAP Dataset'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcRM1Hn0HaKt",
        "outputId": "213672de-b600-4307-f7b8-2adff6c39873"
      },
      "source": [
        "f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=images_left_bgr + images_right_bgr)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize(f'drive/MyDrive/all_images_bgr_{Dataset}.h5')/1.e6,'MB')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HDF5  w/o comp.: 15.441627025604248 [s] ... size 1717.96928 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7daORhhPUlY"
      },
      "source": [
        "'''\n",
        "f=h5.File(f'drive/MyDrive/all_images_gray_{Dataset}.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=images_left + images_right)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize(f'drive/MyDrive/all_images_gray_{Dataset}.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuMrRmX4PUe1"
      },
      "source": [
        "del images_left_bgr,images_right_bgr"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu0CPw_HPUXk"
      },
      "source": [
        "#images_left_bgr_no_enhance = []\n",
        "#images_right_bgr_no_enhance = []\n",
        "\n",
        "#for file in tqdm(left_files_path):\n",
        "#  left_image_sat= cv2.imread(file)\n",
        "#  left_img = cv2.resize(left_image_sat,None,fx=0.35, fy=0.35, interpolation = cv2.INTER_CUBIC)\n",
        "#  images_left_bgr_no_enhance.append(left_img)\n",
        "\n",
        "#for file in tqdm(right_files_path):\n",
        "# right_image_sat= cv2.imread(file)\n",
        "#  right_img = cv2.resize(right_image_sat,None,fx=0.35,fy=0.35, interpolation = cv2.INTER_CUBIC)\n",
        "#  images_right_bgr_no_enhance.append(right_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw1048ZtPUPW"
      },
      "source": [
        "from timeit import default_timer as timer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEz_15CQRVZ4"
      },
      "source": [
        "time_all = []"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6qDnTfWRnoy"
      },
      "source": [
        "num_kps_sift = []\n",
        "num_kps_brisk = []\n",
        "num_kps_agast = []\n",
        "num_kps_kaze = []\n",
        "num_kps_akaze = []\n",
        "num_kps_orb = []\n",
        "num_kps_mser = []\n",
        "num_kps_daisy = []\n",
        "num_kps_surfsift = []\n",
        "num_kps_fast = []\n",
        "num_kps_freak = []\n",
        "num_kps_gftt = []\n",
        "num_kps_star = []\n",
        "num_kps_surf = []\n",
        "num_kps_rootsift = []\n",
        "num_kps_superpoint = []\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7DFuxxfT2Bq"
      },
      "source": [
        "images_left_bgr = []\n",
        "images_right_bgr = []"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzUMd-6AvOu"
      },
      "source": [
        "BRISK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEgZRVa2AtnF"
      },
      "source": [
        "'''\n",
        "Threshl=60;\n",
        "Octaves=6; \n",
        "#PatternScales=1.0f;\n",
        "\n",
        "start = timer()\n",
        "\n",
        "brisk = cv2.BRISK_create(Threshl,Octaves)\n",
        "\n",
        "\n",
        "keypoints_all_left_brisk = []\n",
        "descriptors_all_left_brisk = []\n",
        "points_all_left_brisk=[]\n",
        "\n",
        "keypoints_all_right_brisk = []\n",
        "descriptors_all_right_brisk = []\n",
        "points_all_right_brisk=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()  \n",
        "  kpt = brisk.detect(imgs,None)\n",
        "  kpt,descrip =  brisk.compute(imgs, kpt)\n",
        "  keypoints_all_left_brisk.append(kpt)\n",
        "  descriptors_all_left_brisk.append(descrip)\n",
        "  #points_all_left_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = brisk.detect(imgs,None)\n",
        "  kpt,descrip =  brisk.compute(imgs, kpt)\n",
        "  keypoints_all_right_brisk.append(kpt)\n",
        "  descriptors_all_right_brisk.append(descrip)\n",
        "  #points_all_right_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-kBwl3FAtc2"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_brisk + keypoints_all_right_brisk[1:]):\n",
        "  num_kps_brisk.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toJp9ANJA1B7"
      },
      "source": [
        "'''\n",
        "all_feat_brisk_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_brisk):\n",
        "  all_feat_brisk_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_brisk[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_brisk_left_each.append(temp)\n",
        "  all_feat_brisk_left.append(all_feat_brisk_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCJoIhp3A0-_"
      },
      "source": [
        "'''\n",
        "all_feat_brisk_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_brisk):\n",
        "  all_feat_brisk_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_brisk[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_brisk_right_each.append(temp)\n",
        "  all_feat_brisk_right.append(all_feat_brisk_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W50kDEIyA07K"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_brisk, keypoints_all_right_brisk, descriptors_all_left_brisk, descriptors_all_right_brisk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9phujFrA04X"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_brisk_left.dat', 'wb')\n",
        "pickle.dump(all_feat_brisk_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzyVXsHwA00p"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_brisk_right.dat', 'wb')\n",
        "pickle.dump(all_feat_brisk_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xULi5lVwA0vZ"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_brisk_left, all_feat_brisk_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC1agZ6BBZo"
      },
      "source": [
        "ORB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ4R8QrDA_Zr"
      },
      "source": [
        "'''\n",
        "orb = cv2.ORB_create(40000)\n",
        "\n",
        "start = timer()\n",
        "\n",
        "\n",
        "keypoints_all_left_orb = []\n",
        "descriptors_all_left_orb = []\n",
        "points_all_left_orb=[]\n",
        "\n",
        "keypoints_all_right_orb = []\n",
        "descriptors_all_right_orb = []\n",
        "points_all_right_orb=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()    \n",
        "  kpt = orb.detect(imgs,None)\n",
        "  kpt,descrip =  orb.compute(imgs, kpt)\n",
        "  keypoints_all_left_orb.append(kpt)\n",
        "  descriptors_all_left_orb.append(descrip)\n",
        "  #points_all_left_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = orb.detect(imgs,None)\n",
        "  kpt,descrip =  orb.compute(imgs, kpt)\n",
        "  keypoints_all_right_orb.append(kpt)\n",
        "  descriptors_all_right_orb.append(descrip)\n",
        "  #points_all_right_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a6mfgoZA_To"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_orb + keypoints_all_right_orb[1:]):\n",
        "  num_kps_orb.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-djChAE5A_Q8"
      },
      "source": [
        "'''\n",
        "all_feat_orb_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_orb):\n",
        "  all_feat_orb_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_orb[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_orb_left_each.append(temp)\n",
        "  all_feat_orb_left.append(all_feat_orb_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SameNN1xA_NJ"
      },
      "source": [
        "'''\n",
        "all_feat_orb_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_orb):\n",
        "  all_feat_orb_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_orb[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_orb_right_each.append(temp)\n",
        "  all_feat_orb_right.append(all_feat_orb_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZewtuV_dA_JC"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_orb, keypoints_all_right_orb, descriptors_all_left_orb, descriptors_all_right_orb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkiRp1evBLeA"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_orb_left.dat', 'wb')\n",
        "pickle.dump(all_feat_orb_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaGMXXk3BLa3"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_orb_right.dat', 'wb')\n",
        "pickle.dump(all_feat_orb_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6U0t9SKBLYX"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_orb_left, all_feat_orb_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpoh-2DHWFSW"
      },
      "source": [
        "KAZE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvn4HVDOvquh"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "kaze = cv2.KAZE_create()\n",
        "\n",
        "\n",
        "keypoints_all_left_kaze = []\n",
        "descriptors_all_left_kaze = []\n",
        "points_all_left_kaze=[]\n",
        "\n",
        "keypoints_all_right_kaze = []\n",
        "descriptors_all_right_kaze = []\n",
        "points_all_right_kaze=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = kaze.detect(imgs,None)\n",
        "  kpt,descrip =  kaze.compute(imgs, kpt)\n",
        "  keypoints_all_left_kaze.append(kpt)\n",
        "  descriptors_all_left_kaze.append(descrip)\n",
        "  #points_all_left_kaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = kaze.detect(imgs,None)\n",
        "  kpt,descrip =  kaze.compute(imgs, kpt)\n",
        "  keypoints_all_right_kaze.append(kpt)\n",
        "  descriptors_all_right_kaze.append(descrip)\n",
        "  #points_all_right_kaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7j0fqIeRVPF"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_kaze + keypoints_all_right_kaze[1:]):\n",
        "  num_kps_kaze.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnxCPMDvRVKW"
      },
      "source": [
        "'''\n",
        "all_feat_kaze_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_kaze):\n",
        "  all_feat_kaze_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_kaze[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_kaze_left_each.append(temp)\n",
        "  all_feat_kaze_left.append(all_feat_kaze_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_rsRaTnRVEq"
      },
      "source": [
        "'''\n",
        "all_feat_kaze_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_kaze):\n",
        "  all_feat_kaze_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_kaze[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_kaze_right_each.append(temp)\n",
        "  all_feat_kaze_right.append(all_feat_kaze_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmnJdV8KRU-6"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_kaze, keypoints_all_right_kaze, descriptors_all_left_kaze, descriptors_all_right_kaze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE-eS6B1RU4M"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_kaze_left.dat', 'wb')\n",
        "pickle.dump(all_feat_kaze_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLSfTjxNRUyE"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_kaze_right.dat', 'wb')\n",
        "pickle.dump(all_feat_kaze_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixbUx2XYRUpu"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_kaze_left, all_feat_kaze_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A6LaNo9BZih"
      },
      "source": [
        "AKAZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jxJlohmVPtb"
      },
      "source": [
        "'''\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2IUSGdVPli"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "akaze = cv2.AKAZE_create()\n",
        "\n",
        "\n",
        "keypoints_all_left_akaze = []\n",
        "descriptors_all_left_akaze = []\n",
        "points_all_left_akaze=[]\n",
        "\n",
        "keypoints_all_right_akaze = []\n",
        "descriptors_all_right_akaze = []\n",
        "points_all_right_akaze=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = akaze.detect(imgs,None)\n",
        "  kpt,descrip =  akaze.compute(imgs, kpt)\n",
        "  keypoints_all_left_akaze.append(kpt)\n",
        "  descriptors_all_left_akaze.append(descrip)\n",
        "  #points_all_left_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = akaze.detect(imgs,None)\n",
        "  kpt,descrip = akaze.compute(imgs, kpt)\n",
        "  keypoints_all_right_akaze.append(kpt)\n",
        "  descriptors_all_right_akaze.append(descrip)\n",
        "  #points_all_right_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efye6tC2Bfg_"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_akaze + keypoints_all_right_akaze[1:]):\n",
        "  num_kps_akaze.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6YLQN2lBfbV"
      },
      "source": [
        "'''\n",
        "all_feat_akaze_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_akaze):\n",
        "  all_feat_akaze_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_akaze[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_akaze_left_each.append(temp)\n",
        "  all_feat_akaze_left.append(all_feat_akaze_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOiE4zUkBfXj"
      },
      "source": [
        "'''\n",
        "all_feat_akaze_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_akaze):\n",
        "  all_feat_akaze_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_akaze[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_akaze_right_each.append(temp)\n",
        "  all_feat_akaze_right.append(all_feat_akaze_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp-XefAKBfUV"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_akaze, keypoints_all_right_akaze, descriptors_all_left_akaze, descriptors_all_right_akaze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbHSQehqBfQN"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_akaze_left.dat', 'wb')\n",
        "pickle.dump(all_feat_akaze_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsZSDbuQBfM9"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_akaze_right.dat', 'wb')\n",
        "pickle.dump(all_feat_akaze_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnV2d_GRBrcH"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_akaze_left, all_feat_akaze_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRpih8uqCSc9"
      },
      "source": [
        "\n",
        "STAR + BRIEF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fttzUuuNCVhp"
      },
      "source": [
        "'''\n",
        "\n",
        "start = timer()\n",
        "\n",
        "star = cv2.xfeatures2d.StarDetector_create()\n",
        "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
        "\n",
        "keypoints_all_left_star = []\n",
        "descriptors_all_left_brief = []\n",
        "points_all_left_star=[]\n",
        "\n",
        "keypoints_all_right_star = []\n",
        "descriptors_all_right_brief = []\n",
        "points_all_right_star=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = star.detect(imgs,None)\n",
        "  kpt,descrip =  brief.compute(imgs, kpt)\n",
        "  keypoints_all_left_star.append(kpt)\n",
        "  descriptors_all_left_brief.append(descrip)\n",
        "  #points_all_left_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = star.detect(imgs,None)\n",
        "  kpt,descrip =  brief.compute(imgs, kpt)\n",
        "  keypoints_all_right_star.append(kpt)\n",
        "  descriptors_all_right_brief.append(descrip)\n",
        "  #points_all_right_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uppvh4UwCVWp"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_star + keypoints_all_right_star[1:]):\n",
        "  num_kps_star.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWif45XBCVT_"
      },
      "source": [
        "'''\n",
        "all_feat_star_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_star):\n",
        "  all_feat_star_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_brief[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_star_left_each.append(temp)\n",
        "  all_feat_star_left.append(all_feat_star_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C68lTIg8CVQy"
      },
      "source": [
        "'''\n",
        "all_feat_star_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_star):\n",
        "  all_feat_star_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_brief[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_star_right_each.append(temp)\n",
        "  all_feat_star_right.append(all_feat_star_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtlJNPa5CVMx"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_star, keypoints_all_right_star, descriptors_all_left_brief, descriptors_all_right_brief"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_SedP3HCVJp"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_star_left.dat', 'wb')\n",
        "pickle.dump(all_feat_star_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK3skKAMCVF2"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_star_right.dat', 'wb')\n",
        "pickle.dump(all_feat_star_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnf8kB3MCU91"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_star_left, all_feat_star_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpFFN3FUCoQi"
      },
      "source": [
        "\n",
        "BRISK + FREAK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFjSnoZdCpMa"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "Threshl=60;\n",
        "Octaves=8; \n",
        "#PatternScales=1.0f;\n",
        "brisk = cv2.BRISK_create(Threshl,Octaves)\n",
        "\n",
        "freak = cv2.xfeatures2d.FREAK_create()\n",
        "keypoints_all_left_freak = []\n",
        "descriptors_all_left_freak = []\n",
        "points_all_left_freak=[]\n",
        "\n",
        "keypoints_all_right_freak = []\n",
        "descriptors_all_right_freak = []\n",
        "points_all_right_freak=[]\n",
        "\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = brisk.detect(imgs)\n",
        "  kpt,descrip =  freak.compute(imgs, kpt)\n",
        "  keypoints_all_left_freak.append(kpt)\n",
        "  descriptors_all_left_freak.append(descrip)\n",
        "  #points_all_left_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = brisk.detect(imgs,None)\n",
        "  kpt,descrip =  freak.compute(imgs, kpt)\n",
        "  keypoints_all_right_freak.append(kpt)\n",
        "  descriptors_all_right_freak.append(descrip)\n",
        "  #points_all_right_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRR-JZgPCqb4"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_freak + keypoints_all_right_freak[1:]):\n",
        "  num_kps_freak.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0suZngLtCqXc"
      },
      "source": [
        "'''\n",
        "all_feat_freak_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_freak):\n",
        "  all_feat_freak_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_freak[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_freak_left_each.append(temp)\n",
        "  all_feat_freak_left.append(all_feat_freak_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6UEvj4-CqSJ"
      },
      "source": [
        "'''\n",
        "all_feat_freak_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_freak):\n",
        "  all_feat_freak_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_freak[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_freak_right_each.append(temp)\n",
        "  all_feat_freak_right.append(all_feat_freak_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53M9yWIsCqIW"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_freak, keypoints_all_right_freak, descriptors_all_left_freak, descriptors_all_right_freak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJxhMG_nDKqP"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_freak_left.dat', 'wb')\n",
        "pickle.dump(all_feat_freak_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxjEc1tDKg5"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_freak_right.dat', 'wb')\n",
        "pickle.dump(all_feat_freak_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNluy3ftDKdI"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_freak_left, all_feat_freak_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk6PEqByDSH0"
      },
      "source": [
        "MSER + SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-9JMRADKW6"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "mser = cv2.MSER_create()\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_mser = []\n",
        "descriptors_all_left_mser = []\n",
        "points_all_left_mser=[]\n",
        "\n",
        "keypoints_all_right_mser = []\n",
        "descriptors_all_right_mser = []\n",
        "points_all_right_mser=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = mser.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_mser.append(kpt)\n",
        "  descriptors_all_left_mser.append(descrip)\n",
        "  #points_all_left_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = mser.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_mser.append(kpt)\n",
        "  descriptors_all_right_mser.append(descrip)\n",
        "  #points_all_right_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BstgiHKDKQV"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_mser + keypoints_all_right_mser[1:]):\n",
        "  num_kps_mser.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptgNXIMFDWyl"
      },
      "source": [
        "'''\n",
        "all_feat_mser_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_mser):\n",
        "  all_feat_mser_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_mser[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_mser_left_each.append(temp)\n",
        "  all_feat_mser_left.append(all_feat_mser_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3CvnEpODWu2"
      },
      "source": [
        "'''\n",
        "all_feat_mser_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_mser):\n",
        "  all_feat_mser_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_mser[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_mser_right_each.append(temp)\n",
        "  all_feat_mser_right.append(all_feat_mser_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQfOmhHHDWrL"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_mser, keypoints_all_right_mser, descriptors_all_left_mser, descriptors_all_right_mser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2okVS6MDWno"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_mser_left.dat', 'wb')\n",
        "pickle.dump(all_feat_mser_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj6quw8eDWkD"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_mser_right.dat', 'wb')\n",
        "pickle.dump(all_feat_mser_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyDwaVtyDWgX"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_mser_left, all_feat_mser_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igrL8pcKJIe-"
      },
      "source": [
        "AGAST + SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qptKPAxHJJnN"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "agast = cv2.AgastFeatureDetector_create(threshold = 40)\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_agast = []\n",
        "descriptors_all_left_agast = []\n",
        "points_all_left_agast=[]\n",
        "\n",
        "keypoints_all_right_agast = []\n",
        "descriptors_all_right_agast = []\n",
        "points_all_right_agast=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = agast.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_agast.append(kpt)\n",
        "  descriptors_all_left_agast.append(descrip)\n",
        "  #points_all_left_agast.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = agast.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_agast.append(kpt)\n",
        "  descriptors_all_right_agast.append(descrip)\n",
        "  #points_all_right_agast.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXEiT-i3JKXc"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_agast + keypoints_all_right_agast[1:]):\n",
        "  num_kps_agast.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-tOYuY3JKUX"
      },
      "source": [
        "'''\n",
        "all_feat_agast_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_agast):\n",
        "  all_feat_agast_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_agast[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_agast_left_each.append(temp)\n",
        "  all_feat_agast_left.append(all_feat_agast_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITuhTLbgJKQf"
      },
      "source": [
        "'''\n",
        "all_feat_agast_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_agast):\n",
        "  all_feat_agast_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_agast[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_agast_right_each.append(temp)\n",
        "  all_feat_agast_right.append(all_feat_agast_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7CTjFFfJKNZ"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_agast, keypoints_all_right_agast, descriptors_all_left_agast, descriptors_all_right_agast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZMjm3RAJKIG"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_agast_left.dat', 'wb')\n",
        "pickle.dump(all_feat_agast_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzYI6EvPJKDP"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_agast_left"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9McFDaRnJXQi"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_agast_right.dat', 'wb')\n",
        "pickle.dump(all_feat_agast_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nslPuEslJXJT"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_agast_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmHJo2qUJcZT"
      },
      "source": [
        "FAST + SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig4yKEIGJbsr"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "\n",
        "fast = cv2.FastFeatureDetector_create(threshold=40)\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_fast = []\n",
        "descriptors_all_left_fast = []\n",
        "points_all_left_fast=[]\n",
        "\n",
        "keypoints_all_right_fast = []\n",
        "descriptors_all_right_fast = []\n",
        "points_all_right_fast=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = fast.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_fast.append(kpt)\n",
        "  descriptors_all_left_fast.append(descrip)\n",
        "  #points_all_left_fast.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = fast.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_fast.append(kpt)\n",
        "  descriptors_all_right_fast.append(descrip)\n",
        "  #points_all_right_fast.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie_WnWkWJgQ_"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_fast + keypoints_all_right_fast[1:]):\n",
        "  num_kps_fast.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3pl_8CWJgLr"
      },
      "source": [
        "'''\n",
        "all_feat_fast_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_fast):\n",
        "  all_feat_fast_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_fast[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_fast_left_each.append(temp)\n",
        "  all_feat_fast_left.append(all_feat_fast_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10-tkbiJgGM"
      },
      "source": [
        "'''\n",
        "all_feat_fast_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_fast):\n",
        "  all_feat_fast_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_fast[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_fast_right_each.append(temp)\n",
        "  all_feat_fast_right.append(all_feat_fast_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTGgthlOJf_N"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_fast, keypoints_all_right_fast, descriptors_all_left_fast, descriptors_all_right_fast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv5YzAjFJpRQ"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_fast_left.dat', 'wb')\n",
        "pickle.dump(all_feat_fast_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5d__0yXJpKO"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_fast_right.dat', 'wb')\n",
        "pickle.dump(all_feat_fast_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOu4VdRgJpDW"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_fast_left, all_feat_fast_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfyxf0FoJvlp"
      },
      "source": [
        "GFTT + SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VnVeTvxJwKg"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "gftt = cv2.GFTTDetector_create()\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_gftt = []\n",
        "descriptors_all_left_gftt = []\n",
        "points_all_left_gftt=[]\n",
        "\n",
        "keypoints_all_right_gftt = []\n",
        "descriptors_all_right_gftt = []\n",
        "points_all_right_gftt=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = gftt.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_gftt.append(kpt)\n",
        "  descriptors_all_left_gftt.append(descrip)\n",
        "  #points_all_left_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = gftt.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_gftt.append(kpt)\n",
        "  descriptors_all_right_gftt.append(descrip)\n",
        "  #points_all_right_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_kOTgxDJ0s6"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_gftt + keypoints_all_right_gftt[1:]):\n",
        "  num_kps_gftt.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUPAYraJ0mu"
      },
      "source": [
        "'''\n",
        "all_feat_gftt_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_gftt):\n",
        "  all_feat_gftt_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_gftt[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_gftt_left_each.append(temp)\n",
        "  all_feat_gftt_left.append(all_feat_gftt_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wXmL7naJ0f9"
      },
      "source": [
        "'''\n",
        "all_feat_gftt_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_gftt):\n",
        "  all_feat_gftt_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_gftt[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_gftt_right_each.append(temp)\n",
        "  all_feat_gftt_right.append(all_feat_gftt_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ8_cQ6KJ0Wl"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_gftt, keypoints_all_right_gftt, descriptors_all_left_gftt, descriptors_all_right_gftt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXPIU4DeJ8AN"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_gftt_left.dat', 'wb')\n",
        "pickle.dump(all_feat_gftt_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLAVNzy-J751"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_gftt_right.dat', 'wb')\n",
        "pickle.dump(all_feat_gftt_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz_JbQe3J7z7"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_gftt_left, all_feat_gftt_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg21YN-4LWMd"
      },
      "source": [
        "DAISY+SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXAjN8jYgeP"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "daisy = cv2.xfeatures2d.DAISY_create()\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_daisy = []\n",
        "descriptors_all_left_daisy = []\n",
        "points_all_left_daisy=[]\n",
        "\n",
        "keypoints_all_right_daisy = []\n",
        "descriptors_all_right_daisy = []\n",
        "points_all_right_daisy=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  daisy.compute(imgs, kpt)\n",
        "  keypoints_all_left_daisy.append(kpt)\n",
        "  descriptors_all_left_daisy.append(descrip)\n",
        "  #points_all_left_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  daisy.compute(imgs, kpt)\n",
        "  keypoints_all_right_daisy.append(kpt)\n",
        "  descriptors_all_right_daisy.append(descrip)\n",
        "  #points_all_right_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDJTpGnJYgXy"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_daisy + keypoints_all_right_daisy[1:]):\n",
        "  num_kps_daisy.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTp2Q-HdYgTH"
      },
      "source": [
        "'''\n",
        "all_feat_daisy_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_daisy):\n",
        "  all_feat_daisy_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_daisy[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_daisy_left_each.append(temp)\n",
        "  all_feat_daisy_left.append(all_feat_daisy_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqgObNhQYuhS"
      },
      "source": [
        "'''\n",
        "all_feat_daisy_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_daisy):\n",
        "  all_feat_daisy_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_daisy[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_daisy_right_each.append(temp)\n",
        "  all_feat_daisy_right.append(all_feat_daisy_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vidqbr5UYufD"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_daisy, keypoints_all_right_daisy, descriptors_all_left_daisy, descriptors_all_right_daisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RbfnN6XYuYQ"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_daisy_left.dat', 'wb')\n",
        "pickle.dump(all_feat_daisy_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia8arOghYuQP"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_daisy_right.dat', 'wb')\n",
        "pickle.dump(all_feat_daisy_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44W95xPWYuMD"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_daisy_left, all_feat_daisy_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnpjEZcGA1j_"
      },
      "source": [
        "SURF + SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIZpQjn5A2CE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5912121-4285-4be7-8fc9-87573b21b72f"
      },
      "source": [
        "\n",
        "start = timer()\n",
        "\n",
        "surf = cv2.xfeatures2d.SURF_create(hessianThreshold = 30, nOctaves = 2, nOctaveLayers = 2)\n",
        "sift = cv2.xfeatures2d.SIFT_create(nfeatures = 500, nOctaveLayers = 2, contrastThreshold = 0.02, edgeThreshold = 6)\n",
        "\n",
        "keypoints_all_left_surfsift = []\n",
        "descriptors_all_left_surfsift = []\n",
        "points_all_left_surfsift=[]\n",
        "\n",
        "keypoints_all_right_surfsift = []\n",
        "descriptors_all_right_surfsift = []\n",
        "points_all_right_surfsift=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = surf.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_surfsift.append(kpt)\n",
        "  descriptors_all_left_surfsift.append(descrip)\n",
        "  #points_all_left_surfsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = surf.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_surfsift.append(kpt)\n",
        "  descriptors_all_right_surfsift.append(descrip)\n",
        "  #points_all_right_surfsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26/26 [30:43<00:00, 70.92s/it]\n",
            "100%|██████████| 25/25 [27:17<00:00, 65.52s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-si9W07A2q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8becf3-0e0d-422d-dd2e-db03f7e05fea"
      },
      "source": [
        "\n",
        "for j in tqdm(keypoints_all_left_surfsift + keypoints_all_right_surfsift[1:]):\n",
        "  num_kps_surfsift.append(len(j))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 265798.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKXUSzbfA2Zx"
      },
      "source": [
        "\n",
        "all_feat_surfsift_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_surfsift):\n",
        "  all_feat_surfsift_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_surfsift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_surfsift_left_each.append(temp)\n",
        "  all_feat_surfsift_left.append(all_feat_surfsift_left_each)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA3GBpZABBjl"
      },
      "source": [
        "\n",
        "all_feat_surfsift_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_surfsift):\n",
        "  all_feat_surfsift_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_surfsift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_surfsift_right_each.append(temp)\n",
        "  all_feat_surfsift_right.append(all_feat_surfsift_right_each)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Xlu1ORBBeW"
      },
      "source": [
        "\n",
        "del keypoints_all_left_surfsift, keypoints_all_right_surfsift, descriptors_all_left_surfsift, descriptors_all_right_surfsift"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SaqEJ1nBBYu"
      },
      "source": [
        "\n",
        "import pickle\n",
        "Fdb = open('all_feat_surfsift_left.dat', 'wb')\n",
        "pickle.dump(all_feat_surfsift_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbUvyVuZBBS-"
      },
      "source": [
        "\n",
        "import pickle\n",
        "Fdb = open('all_feat_surfsift_right.dat', 'wb')\n",
        "pickle.dump(all_feat_surfsift_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNYxgZBBBNF"
      },
      "source": [
        "\n",
        "del Fdb, all_feat_surfsift_left, all_feat_surfsift_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G9byE4MBLnF"
      },
      "source": [
        "SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-CBqB4JBBHG"
      },
      "source": [
        "'''\n",
        "print(len(left_files_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tcYtoM5BOhe"
      },
      "source": [
        "'''\n",
        "print(len(right_files_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3JjADHCBOeI"
      },
      "source": [
        "# H5 file w/o compression\n",
        "#t0=time.time()\n",
        "#f=h5.File('drive/MyDrive/all_images_bgr_sift.h5','r')\n",
        "#print('HDF5  w/o comp.: data shape =',len(f['data'][0]),time.time()-t0,'[s]')\n",
        "#f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq3pEMS_BOa9"
      },
      "source": [
        "#del f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8m3R7gIBOXf"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create(nfeatures = 1000, nOctaveLayers = 3, contrastThreshold = 0.06, edgeThreshold = 12)\n",
        "keypoints_all_left_sift = []\n",
        "descriptors_all_left_sift = []\n",
        "points_all_left_sift=[]\n",
        "\n",
        "keypoints_all_right_sift = []\n",
        "descriptors_all_right_sift = []\n",
        "points_all_right_sift=[]\n",
        "\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()\n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_sift.append(kpt)\n",
        "  descriptors_all_left_sift.append(descrip)\n",
        "  #points_all_left_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()\n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_sift.append(kpt)\n",
        "  descriptors_all_right_sift.append(descrip)\n",
        "  #points_all_right_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_q6BV-BOTE"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_sift + keypoints_all_right_sift[1:]):\n",
        "  num_kps_sift.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaOGm1MTBOPJ"
      },
      "source": [
        "'''\n",
        "all_feat_sift_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_sift):\n",
        "  all_feat_sift_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_sift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_sift_left_each.append(temp)\n",
        "  all_feat_sift_left.append(all_feat_sift_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTLEBcppBOJm"
      },
      "source": [
        "'''\n",
        "all_feat_sift_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_sift):\n",
        "  all_feat_sift_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_sift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_sift_right_each.append(temp)\n",
        "  all_feat_sift_right.append(all_feat_sift_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuzpJ4aWBcFv"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_sift, keypoints_all_right_sift, descriptors_all_left_sift, descriptors_all_right_sift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keX-zbR5BcAC"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_sift_left.dat', 'wb')\n",
        "pickle.dump(all_feat_sift_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzV0lBZnBb3_"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_sift_right.dat', 'wb')\n",
        "pickle.dump(all_feat_sift_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsrOHWZcBbwW"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_sift_left, all_feat_sift_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-hRX8jrBkrp"
      },
      "source": [
        "SURF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql4NScZ1Biut"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "surf  = cv2.xfeatures2d.SURF_create(upright=1)\n",
        "keypoints_all_left_surf = []\n",
        "descriptors_all_left_surf = []\n",
        "points_all_left_surf=[]\n",
        "\n",
        "keypoints_all_right_surf = []\n",
        "descriptors_all_right_surf = []\n",
        "points_all_right_surf=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()  \n",
        "  kpt = surf.detect(imgs,None)\n",
        "  kpt,descrip =  surf.compute(imgs, kpt)\n",
        "  keypoints_all_left_surf.append(kpt)\n",
        "  descriptors_all_left_surf.append(descrip)\n",
        "  #points_all_left_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = surf.detect(imgs,None)\n",
        "  kpt,descrip =  surf.compute(imgs, kpt)\n",
        "  keypoints_all_right_surf.append(kpt)\n",
        "  descriptors_all_right_surf.append(descrip)\n",
        "  #points_all_right_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebC3Q3MBBm4O"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_surf + keypoints_all_right_surf[1:]):\n",
        "  num_kps_surf.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3tsk0eJBm0o"
      },
      "source": [
        "'''\n",
        "all_feat_surf_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_surf):\n",
        "  all_feat_surf_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_surf[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_surf_left_each.append(temp)\n",
        "  all_feat_surf_left.append(all_feat_surf_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-RykyRjBmx1"
      },
      "source": [
        "'''\n",
        "all_feat_surf_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_surf):\n",
        "  all_feat_surf_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_surf[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_surf_right_each.append(temp)\n",
        "  all_feat_surf_right.append(all_feat_surf_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBXM6S9VBmuh"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_surf, keypoints_all_right_surf, descriptors_all_left_surf, descriptors_all_right_surf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnYtZPMHBmqx"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_surf_left.dat', 'wb')\n",
        "pickle.dump(all_feat_surf_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYl890XMByIG"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_surf_right.dat', 'wb')\n",
        "pickle.dump(all_feat_surf_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIu9u7gPB0Mo"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_surf_left, all_feat_surf_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X-Xmis2Y9k8"
      },
      "source": [
        "ROOTSIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq-AUPpAY5CN"
      },
      "source": [
        "'''\n",
        "class RootSIFT:\n",
        "  def __init__(self):\n",
        "    # initialize the SIFT feature extractor\n",
        "    #self.extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
        "    self.sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "  def compute(self, image, kps, eps=1e-7):\n",
        "    # compute SIFT descriptors\n",
        "    (kps, descs) = self.sift.compute(image, kps)\n",
        "\n",
        "    # if there are no keypoints or descriptors, return an empty tuple\n",
        "    if len(kps) == 0:\n",
        "      return ([], None)\n",
        "\n",
        "    # apply the Hellinger kernel by first L1-normalizing, taking the\n",
        "    # square-root, and then L2-normalizing\n",
        "    descs /= (np.linalg.norm(descs, axis=0, ord=2) + eps)\n",
        "    descs /= (descs.sum(axis=0) + eps)\n",
        "    descs = np.sqrt(descs)\n",
        "    #descs /= (np.linalg.norm(descs, axis=0, ord=2) + eps)\n",
        "\n",
        "    # return a tuple of the keypoints and descriptors\n",
        "    return (kps, descs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKVJojBAY49D"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "rootsift = RootSIFT()\n",
        "keypoints_all_left_rootsift = []\n",
        "descriptors_all_left_rootsift = []\n",
        "points_all_left_rootsift=[]\n",
        "\n",
        "keypoints_all_right_rootsift = []\n",
        "descriptors_all_right_rootsift = []\n",
        "points_all_right_rootsift=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()  \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  rootsift.compute(imgs, kpt)\n",
        "  keypoints_all_left_rootsift.append(kpt)\n",
        "  descriptors_all_left_rootsift.append(descrip)\n",
        "  #points_all_left_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  rootsift.compute(imgs, kpt)\n",
        "  keypoints_all_right_rootsift.append(kpt)\n",
        "  descriptors_all_right_rootsift.append(descrip)\n",
        "  #points_all_right_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-pIYbFDY44T"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_rootsift + keypoints_all_right_rootsift[1:]):\n",
        "  num_kps_rootsift.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzaUmPtGY4y3"
      },
      "source": [
        "'''\n",
        "all_feat_rootsift_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_rootsift):\n",
        "  all_feat_rootsift_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_rootsift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_rootsift_left_each.append(temp)\n",
        "  all_feat_rootsift_left.append(all_feat_rootsift_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOz51FvlY4uE"
      },
      "source": [
        "'''\n",
        "all_feat_rootsift_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_rootsift):\n",
        "  all_feat_rootsift_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_rootsift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_rootsift_right_each.append(temp)\n",
        "  all_feat_rootsift_right.append(all_feat_rootsift_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j2VRWDAY4of"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_rootsift, keypoints_all_right_rootsift, descriptors_all_left_rootsift, descriptors_all_right_rootsift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZbKnM1ZY4jI"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_left.dat', 'wb')\n",
        "pickle.dump(all_feat_rootsift_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImEPlaTtY4dc"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_right.dat', 'wb')\n",
        "pickle.dump(all_feat_rootsift_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxznTKgpZM01"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_rootsift_left, all_feat_rootsift_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMidlSPsB6XZ"
      },
      "source": [
        "SuperPoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWZhoEjuZfIv"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/magicleap/SuperPointPretrainedNetwork.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6bD80zQZfCR"
      },
      "source": [
        "'''\n",
        "weights_path = 'SuperPointPretrainedNetwork/superpoint_v1.pth'\n",
        "\n",
        "cuda = 'True'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIiSaHNCCG_i"
      },
      "source": [
        "'''\n",
        "def to_kpts(pts, size=1):\n",
        "  return [cv2.KeyPoint(pt[0], pt[1], size) for pt in pts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHzRz_l5CG6B"
      },
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class SuperPointNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SuperPointNet, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
        "        # Shared Encoder.\n",
        "        self.conv1a = nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1b = nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2a = nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2b = nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3a = nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3b = nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4a = nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4b = nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
        "        # Detector Head.\n",
        "        self.convPa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
        "        self.convPb = nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
        "        # Descriptor Head.\n",
        "        self.convDa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
        "        self.convDb = nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Shared Encoder.\n",
        "        x = self.relu(self.conv1a(x))\n",
        "        x = self.relu(self.conv1b(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2a(x))\n",
        "        x = self.relu(self.conv2b(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3a(x))\n",
        "        x = self.relu(self.conv3b(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv4a(x))\n",
        "        x = self.relu(self.conv4b(x))\n",
        "        # Detector Head.\n",
        "        cPa = self.relu(self.convPa(x))\n",
        "        semi = self.convPb(cPa)\n",
        "        # Descriptor Head.\n",
        "        cDa = self.relu(self.convDa(x))\n",
        "        desc = self.convDb(cDa)\n",
        "        dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
        "        desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
        "        return semi, desc\n",
        "\n",
        "\n",
        "class SuperPointFrontend(object):\n",
        "    def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,cuda=True):\n",
        "        self.name = 'SuperPoint'\n",
        "        self.cuda = cuda\n",
        "        self.nms_dist = nms_dist\n",
        "        self.conf_thresh = conf_thresh\n",
        "        self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
        "        self.cell = 8 # Size of each output cell. Keep this fixed.\n",
        "        self.border_remove = 4 # Remove points this close to the border.\n",
        "\n",
        "        # Load the network in inference mode.\n",
        "        self.net = SuperPointNet()\n",
        "        if cuda:\n",
        "          # Train on GPU, deploy on GPU.\n",
        "            self.net.load_state_dict(torch.load(weights_path))\n",
        "            self.net = self.net.cuda()\n",
        "        else:\n",
        "          # Train on GPU, deploy on CPU.\n",
        "            self.net.load_state_dict(torch.load(weights_path, map_location=lambda storage, loc: storage))\n",
        "        self.net.eval()\n",
        "\n",
        "    def nms_fast(self, in_corners, H, W, dist_thresh):\n",
        "\n",
        "        grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
        "        inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
        "        # Sort by confidence and round to nearest int.\n",
        "        inds1 = np.argsort(-in_corners[2,:])\n",
        "        corners = in_corners[:,inds1]\n",
        "        rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
        "        # Check for edge case of 0 or 1 corners.\n",
        "        if rcorners.shape[1] == 0:\n",
        "            return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
        "        if rcorners.shape[1] == 1:\n",
        "            out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
        "            return out, np.zeros((1)).astype(int)\n",
        "        # Initialize the grid.\n",
        "        for i, rc in enumerate(rcorners.T):\n",
        "            grid[rcorners[1,i], rcorners[0,i]] = 1\n",
        "            inds[rcorners[1,i], rcorners[0,i]] = i\n",
        "        # Pad the border of the grid, so that we can NMS points near the border.\n",
        "        pad = dist_thresh\n",
        "        grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
        "        # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
        "        count = 0\n",
        "        for i, rc in enumerate(rcorners.T):\n",
        "          # Account for top and left padding.\n",
        "            pt = (rc[0]+pad, rc[1]+pad)\n",
        "            if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
        "                grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
        "                grid[pt[1], pt[0]] = -1\n",
        "                count += 1\n",
        "        # Get all surviving -1's and return sorted array of remaining corners.\n",
        "        keepy, keepx = np.where(grid==-1)\n",
        "        keepy, keepx = keepy - pad, keepx - pad\n",
        "        inds_keep = inds[keepy, keepx]\n",
        "        out = corners[:, inds_keep]\n",
        "        values = out[-1, :]\n",
        "        inds2 = np.argsort(-values)\n",
        "        out = out[:, inds2]\n",
        "        out_inds = inds1[inds_keep[inds2]]\n",
        "        return out, out_inds\n",
        "\n",
        "    def run(self, img):\n",
        "        assert img.ndim == 2 #Image must be grayscale.\n",
        "        assert img.dtype == np.float32 #Image must be float32.\n",
        "        H, W = img.shape[0], img.shape[1]\n",
        "        inp = img.copy()\n",
        "        inp = (inp.reshape(1, H, W))\n",
        "        inp = torch.from_numpy(inp)\n",
        "        inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
        "        if self.cuda:\n",
        "            inp = inp.cuda()\n",
        "        # Forward pass of network.\n",
        "        outs = self.net.forward(inp)\n",
        "        semi, coarse_desc = outs[0], outs[1]\n",
        "        # Convert pytorch -> numpy.\n",
        "        semi = semi.data.cpu().numpy().squeeze()\n",
        "        \n",
        "        # --- Process points.\n",
        "        dense = np.exp(semi) # Softmax.\n",
        "        dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
        "        nodust = dense[:-1, :, :]\n",
        "        # Reshape to get full resolution heatmap.\n",
        "        Hc = int(H / self.cell)\n",
        "        Wc = int(W / self.cell)\n",
        "        nodust = np.transpose(nodust, [1, 2, 0])\n",
        "        heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
        "        heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
        "        heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell]) \n",
        "        prob_map = heatmap/np.sum(np.sum(heatmap))\n",
        "        \n",
        "        return heatmap, coarse_desc\n",
        "\n",
        "\n",
        "    def key_pt_sampling(self, img, heat_map, coarse_desc, sampled):\n",
        "        \n",
        "        H, W = img.shape[0], img.shape[1]\n",
        "\n",
        "        xs, ys = np.where(heat_map >= self.conf_thresh) # Confidence threshold.\n",
        "        if len(xs) == 0:\n",
        "            return np.zeros((3, 0)), None, None\n",
        "        print(\"number of pts selected :\", len(xs))\n",
        "        \n",
        "        \n",
        "        pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
        "        pts[0, :] = ys\n",
        "        pts[1, :] = xs\n",
        "        pts[2, :] = heat_map[xs, ys]\n",
        "        pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
        "        inds = np.argsort(pts[2,:])\n",
        "        pts = pts[:,inds[::-1]] # Sort by confidence.\n",
        "        bord = self.border_remove\n",
        "        toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
        "        toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
        "        toremove = np.logical_or(toremoveW, toremoveH)\n",
        "        pts = pts[:, ~toremove]\n",
        "        pts = pts[:,0:sampled] #we take 2000 keypoints with highest probability from heatmap for our benchmark\n",
        "        \n",
        "        # --- Process descriptor.\n",
        "        D = coarse_desc.shape[1]\n",
        "        if pts.shape[1] == 0:\n",
        "            desc = np.zeros((D, 0))\n",
        "        else:\n",
        "          # Interpolate into descriptor map using 2D point locations.\n",
        "            samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
        "            samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
        "            samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
        "            samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
        "            samp_pts = samp_pts.view(1, 1, -1, 2)\n",
        "            samp_pts = samp_pts.float()\n",
        "            if self.cuda:\n",
        "                samp_pts = samp_pts.cuda()            \n",
        "            desc = nn.functional.grid_sample(coarse_desc, samp_pts)\n",
        "            desc = desc.data.cpu().numpy().reshape(D, -1)\n",
        "            desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
        "\n",
        "            \n",
        "        return pts, desc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_NTKvX5CG0h"
      },
      "source": [
        "'''\n",
        "print('Loading pre-trained network.')\n",
        "# This class runs the SuperPoint network and processes its outputs.\n",
        "fe = SuperPointFrontend(weights_path=weights_path,nms_dist = 3,conf_thresh = 0.01,nn_thresh=0.5)\n",
        "print('Successfully loaded pre-trained network.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zARa2vimCGu2"
      },
      "source": [
        "'''\n",
        "start = timer()\n",
        "\n",
        "keypoints_all_left_superpoint = []\n",
        "descriptors_all_left_superpoint = []\n",
        "points_all_left_superpoint=[]\n",
        "\n",
        "keypoints_all_right_superpoint = []\n",
        "descriptors_all_right_superpoint = []\n",
        "points_all_right_superpoint=[]\n",
        "\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_gray_{Dataset}.h5','r')\n",
        "  lfpth = f['data'][cnt]\n",
        "  f.close()  \n",
        "  heatmap1, coarse_desc1 = fe.run(lfpth)\n",
        "  pts_1, desc_1 = fe.key_pt_sampling(lfpth, heatmap1, coarse_desc1, 80000) #Getting keypoints and descriptors for 1st image\n",
        "\n",
        "  keypoints_all_left_superpoint.append(to_kpts(pts_1.T))\n",
        "  descriptors_all_left_superpoint.append(desc_1.T)\n",
        "  #points_all_left_superpoint.append(pts_1.T)\n",
        "\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File(f'drive/MyDrive/all_images_gray_{Dataset}.h5','r')\n",
        "  rfpth = f['data'][cnt]\n",
        "  f.close()  \n",
        "  heatmap1, coarse_desc1 = fe.run(rfpth)\n",
        "  pts_1, desc_1 = fe.key_pt_sampling(rfpth, heatmap1, coarse_desc1, 80000) #Getting keypoints and descriptors for 1st image\n",
        "\n",
        "  keypoints_all_right_superpoint.append(to_kpts(pts_1.T))\n",
        "  descriptors_all_right_superpoint.append(desc_1.T)\n",
        "  #points_all_right_superpoint.append(pts_1.T)\n",
        "\n",
        "end = timer()\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GShtFy1qCGpV"
      },
      "source": [
        "'''\n",
        "for j in tqdm(keypoints_all_left_superpoint + keypoints_all_right_superpoint[1:]):\n",
        "  num_kps_superpoint.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V3c3iWJCGjT"
      },
      "source": [
        "'''\n",
        "all_feat_superpoint_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_superpoint):\n",
        "  all_feat_superpoint_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_superpoint[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_superpoint_left_each.append(temp)\n",
        "  all_feat_superpoint_left.append(all_feat_superpoint_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygLEJB5eCTXV"
      },
      "source": [
        "'''\n",
        "all_feat_superpoint_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_superpoint):\n",
        "  all_feat_superpoint_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_superpoint[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_superpoint_right_each.append(temp)\n",
        "  all_feat_superpoint_right.append(all_feat_superpoint_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DHtCaFKCTSQ"
      },
      "source": [
        "'''\n",
        "del keypoints_all_left_superpoint, keypoints_all_right_superpoint, descriptors_all_left_superpoint, descriptors_all_right_superpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwLhhM_tCTKq"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_superpoint_left.dat', 'wb')\n",
        "pickle.dump(all_feat_superpoint_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2XRYMrfCYhy"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_superpoint_right.dat', 'wb')\n",
        "pickle.dump(all_feat_superpoint_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCWo--YTCYaM"
      },
      "source": [
        "'''\n",
        "del Fdb, all_feat_superpoint_left, all_feat_superpoint_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFAdEmiACcst"
      },
      "source": [
        "Total Matches,Robust Matches and Homography Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1HvLX8HSWoQ"
      },
      "source": [
        "def compute_homography_fast(matched_pts1, matched_pts2,thresh=4):\n",
        "    #matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
        "    #matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
        "\n",
        "    # Estimate the homography between the matches using RANSAC\n",
        "    H, inliers = cv2.findHomography(matched_pts1,\n",
        "                                    matched_pts2,\n",
        "                                    cv2.RANSAC, ransacReprojThreshold =thresh, maxIters=3000)\n",
        "    inliers = inliers.flatten()\n",
        "    return H, inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rQ1tDx0SWf9"
      },
      "source": [
        "def compute_homography_fast_other(matched_pts1, matched_pts2):\n",
        "    #matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
        "    #matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
        "\n",
        "    # Estimate the homography between the matches using RANSAC\n",
        "    H, inliers = cv2.findHomography(matched_pts1,\n",
        "                                    matched_pts2,\n",
        "                                    0)\n",
        "    inliers = inliers.flatten()\n",
        "    return H, inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q8lSaVqTQIE"
      },
      "source": [
        "def get_Hmatrix(imgs,keypts,pts,descripts,ratio=0.75,thresh=4,use_lowe=True,disp=False,no_ransac=False,binary=False):\n",
        "  lff1 = descripts[0]\n",
        "  lff = descripts[1]\n",
        "\n",
        "  if use_lowe==False:\n",
        "    #FLANN_INDEX_KDTREE = 2\n",
        "    #index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    #search_params = dict(checks=50)\n",
        "    #flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    #flann = cv2.BFMatcher()\n",
        "    if binary==True:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "    else:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "\n",
        "\n",
        "    #matches_lf1_lf = flann.knnMatch(lff1, lff, k=2)\n",
        "    matches_4 = bf.knnMatch(lff1, lff,k=2)\n",
        "    matches_lf1_lf = []\n",
        "\n",
        "\n",
        "    print(\"\\nNumber of matches\",len(matches_4))\n",
        "    '''\n",
        "    matches_4 = []\n",
        "    ratio = ratio\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Lowe’s ratio test)\n",
        "      #if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "      matches_4.append(m[0])\n",
        "    '''\n",
        "    print(\"Number of matches After Lowe's Ratio\",len(matches_4))\n",
        "  else:\n",
        "    FLANN_INDEX_KDTREE = 2\n",
        "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    search_params = dict(checks=50)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    if binary==True:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "    else:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "\n",
        "\n",
        "    matches_lf1_lf = flann.knnMatch(lff1, lff, k=2)\n",
        "    #matches_lf1_lf = bf.knnMatch(lff1, lff,k=2)\n",
        "\n",
        "\n",
        "    print(\"\\nNumber of matches\",len(matches_lf1_lf))\n",
        "    matches_4 = []\n",
        "    ratio = ratio\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Lowe’s ratio test)\n",
        "      if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "        matches_4.append(m[0])\n",
        "  \n",
        "    print(\"Number of matches After Lowe's Ratio\",len(matches_4))\n",
        "\n",
        "\n",
        "  \n",
        "  matches_idx = np.array([m.queryIdx for m in matches_4])\n",
        "  imm1_pts = np.array([keypts[0][idx].pt for idx in matches_idx])\n",
        "  matches_idx = np.array([m.trainIdx for m in matches_4])\n",
        "  imm2_pts = np.array([keypts[1][idx].pt for idx in matches_idx])\n",
        "  '''\n",
        "  # Estimate homography 1\n",
        "  #Compute H1\n",
        "  # Estimate homography 1\n",
        "  #Compute H1\n",
        "  imm1_pts=np.empty((len(matches_4),2))\n",
        "  imm2_pts=np.empty((len(matches_4),2))\n",
        "  for i in range(0,len(matches_4)):\n",
        "    m = matches_4[i]\n",
        "    (a_x, a_y) = keypts[0][m.queryIdx].pt\n",
        "    (b_x, b_y) = keypts[1][m.trainIdx].pt\n",
        "    imm1_pts[i]=(a_x, a_y)\n",
        "    imm2_pts[i]=(b_x, b_y)    \n",
        "  H=compute_Homography(imm1_pts,imm2_pts) \n",
        "  #Robustly estimate Homography 1 using RANSAC\n",
        "  Hn, best_inliers=RANSAC_alg(keypts[0] ,keypts[1], matches_4,  nRANSAC=1000, RANSACthresh=6)\n",
        "  '''\n",
        "  \n",
        "  if no_ransac==True:\n",
        "    Hn,inliers = compute_homography_fast_other(imm1_pts,imm2_pts)\n",
        "  else:\n",
        "    Hn,inliers = compute_homography_fast(imm1_pts,imm2_pts,thresh)  \n",
        "\n",
        "  inlier_matchset = np.array(matches_4)[inliers.astype(bool)].tolist()\n",
        "  print(\"Number of Robust matches\",len(inlier_matchset))\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  if len(inlier_matchset)<25:\n",
        "    matches_4 = []\n",
        "    ratio = 0.85\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Lowe’s ratio test)\n",
        "      if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "          matches_4.append(m[0])\n",
        "    print(\"Number of matches After Lowe's Ratio New\",len(matches_4))\n",
        "  \n",
        "    matches_idx = np.array([m.queryIdx for m in matches_4])\n",
        "    imm1_pts = np.array([keypts[0][idx].pt for idx in matches_idx])\n",
        "    matches_idx = np.array([m.trainIdx for m in matches_4])\n",
        "    imm2_pts = np.array([keypts[1][idx].pt for idx in matches_idx])\n",
        "    Hn,inliers = compute_homography_fast(imm1_pts,imm2_pts)  \n",
        "    inlier_matchset = np.array(matches_4)[inliers.astype(bool)].tolist()\n",
        "    print(\"Number of Robust matches New\",len(inlier_matchset))\n",
        "    print(\"\\n\")    \n",
        "  \n",
        "  #H=compute_Homography(imm1_pts,imm2_pts) \n",
        "  #Robustly estimate Homography 1 using RANSAC\n",
        "  #Hn=RANSAC_alg(keypts[0] ,keypts[1], matches_4,  nRANSAC=1500, RANSACthresh=6)\n",
        "\n",
        "  #global inlier_matchset   \n",
        "  \n",
        "  if disp==True:\n",
        "    dispimg1=cv2.drawMatches(imgs[0], keypts[0], imgs[1], keypts[1], inlier_matchset, None,flags=2)\n",
        "    displayplot(dispimg1,'Robust Matching between Reference Image and Right Image ')\n",
        "  \n",
        "  \n",
        "  return Hn/Hn[2,2], len(matches_lf1_lf), len(inlier_matchset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbSaUB-hTQDG"
      },
      "source": [
        "def get_Hmatrix_rfnet(imgs,pts,descripts,disp=True):\n",
        "\n",
        "  des1 = descripts[0]\n",
        "  des2 = descripts[1]\n",
        "\n",
        "  kp1 = pts[0]\n",
        "  kp2 = pts[1]\n",
        "\n",
        "\n",
        "  predict_label, nn_kp2 = nearest_neighbor_distance_ratio_match(des1, des2, kp2, 0.7)\n",
        "  idx = predict_label.nonzero().view(-1)\n",
        "  mkp1 = kp1.index_select(dim=0, index=idx.long())  # predict match keypoints in I1\n",
        "  mkp2 = nn_kp2.index_select(dim=0, index=idx.long())  # predict match keypoints in I2\n",
        "\n",
        "  #img1, img2 = reverse_img(img1), reverse_img(img2)\n",
        "  keypoints1 = list(map(to_cv2_kp, mkp1))\n",
        "  keypoints2 = list(map(to_cv2_kp, mkp2))\n",
        "  DMatch = list(map(to_cv2_dmatch, np.arange(0, len(keypoints1))))\n",
        "\n",
        "  imm1_pts=np.empty((len(DMatch),2))\n",
        "  imm2_pts=np.empty((len(DMatch),2))\n",
        "  for i in range(0,len(DMatch)):\n",
        "    m = DMatch[i]\n",
        "    (a_x, a_y) = keypoints1[m.queryIdx].pt\n",
        "    (b_x, b_y) = keypoints2[m.trainIdx].pt\n",
        "    imm1_pts[i]=(a_x, a_y)\n",
        "    imm2_pts[i]=(b_x, b_y)    \n",
        "  H=compute_Homography_fast(imm1_pts,imm2_pts) \n",
        "\n",
        "\n",
        "  if disp==True:\n",
        "    dispimg1 = cv2.drawMatches(imgs[0], keypoints1, imgs[1], keypoints2, DMatch, None)\n",
        "    displayplot(dispimg1,'Robust Matching between Reference Image and Right Image ')\n",
        "\n",
        "\n",
        "  return H/H[2,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxmB14K1IGji"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_brisk_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_brisk = []\n",
        "descriptors_all_left_brisk = []\n",
        "points_all_left_brisk = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_brisk.append(keypoints_each)\n",
        "  descriptors_all_left_brisk.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbsNJSsCIGfP"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_brisk_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_brisk = []\n",
        "descriptors_all_right_brisk = []\n",
        "points_all_right_brisk = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_brisk.append(keypoints_each)\n",
        "  descriptors_all_right_brisk.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvc4MelmIGaK"
      },
      "source": [
        "'''\n",
        "H_left_brisk = []\n",
        "H_right_brisk = []\n",
        "\n",
        "num_matches_brisk = []\n",
        "num_good_matches_brisk = []\n",
        "\n",
        "images_left_bgr = []\n",
        "images_right_bgr = []\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_brisk[j:j+2][::-1],points_all_left_brisk[j:j+2][::-1],descriptors_all_left_brisk[j:j+2][::-1],0.7,3,use_lowe=True,binary=True)\n",
        "  H_left_brisk.append(H_a)\n",
        "  num_matches_brisk.append(matches)\n",
        "  num_good_matches_brisk.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_brisk[j:j+2][::-1],points_all_right_brisk[j:j+2][::-1],descriptors_all_right_brisk[j:j+2][::-1],0.7,3,use_lowe=True,binary=True)\n",
        "  H_right_brisk.append(H_a)\n",
        "  num_matches_brisk.append(matches)\n",
        "  num_good_matches_brisk.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8T74TJ4IGU9"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_brisk_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_brisk)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_brisk_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_nFQdxmIGOU"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_brisk_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_brisk)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_brisk_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mkl11ZfIGGP"
      },
      "source": [
        "'''\n",
        "del H_left_brisk, H_right_brisk,keypoints_all_left_brisk, keypoints_all_right_brisk, descriptors_all_left_brisk, descriptors_all_right_brisk, points_all_left_brisk, points_all_right_brisk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bbuf1pWIF7k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hAvJVtyCpgk"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_sift_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_sift = []\n",
        "descriptors_all_left_sift = []\n",
        "\n",
        "\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_sift.append(keypoints_each)\n",
        "  descriptors_all_left_sift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYZ8M-JGCpdG"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_sift_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_sift = []\n",
        "descriptors_all_right_sift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_sift.append(keypoints_each)\n",
        "  descriptors_all_right_sift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzLUr-JVCpaE"
      },
      "source": [
        "'''\n",
        "H_left_sift = []\n",
        "H_right_sift = []\n",
        "\n",
        "num_matches_sift = []\n",
        "num_good_matches_sift = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_sift[j:j+2][::-1],points_all_left_sift[j:j+2][::-1],descriptors_all_left_sift[j:j+2][::-1],0.75)\n",
        "  H_left_sift.append(H_a)\n",
        "  num_matches_sift.append(matches)\n",
        "  num_good_matches_sift.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_sift[j:j+2][::-1],points_all_right_sift[j:j+2][::-1],descriptors_all_right_sift[j:j+2][::-1],0.75)\n",
        "  H_right_sift.append(H_a)\n",
        "  num_matches_sift.append(matches)\n",
        "  num_good_matches_sift.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0ZfgXvFCpXA"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_sift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_sift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_sift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5aeoRmrCpUD"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_sift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_sift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_sift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm6OxmVOCpOq"
      },
      "source": [
        "'''\n",
        "del H_left_sift, H_right_sift,keypoints_all_left_sift, keypoints_all_right_sift, descriptors_all_left_sift, descriptors_all_right_sift, points_all_left_sift, points_all_right_sift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq9_ipIrCpL0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrD8MHF3CpJT"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_fast_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_fast = []\n",
        "descriptors_all_left_fast = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_fast.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_fast.append(keypoints_each)\n",
        "  descriptors_all_left_fast.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgCxWEUpCpGg"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_fast_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_fast = []\n",
        "descriptors_all_right_fast = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_fast.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_fast.append(keypoints_each)\n",
        "  descriptors_all_right_fast.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBGTBep2CpD7"
      },
      "source": [
        "'''\n",
        "H_left_fast = []\n",
        "H_right_fast = []\n",
        "\n",
        "num_matches_fast = []\n",
        "num_good_matches_fast = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_fast[j:j+2][::-1],points_all_left_fast[j:j+2][::-1],descriptors_all_left_fast[j:j+2][::-1],0.9,6)\n",
        "  H_left_fast.append(H_a)\n",
        "  num_matches_fast.append(matches)\n",
        "  num_good_matches_fast.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_fast[j:j+2][::-1],points_all_right_fast[j:j+2][::-1],descriptors_all_right_fast[j:j+2][::-1],0.9,6)\n",
        "  H_right_fast.append(H_a)\n",
        "  num_matches_fast.append(matches)\n",
        "  num_good_matches_fast.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KYrPTE6CpAT"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_fast_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_fast)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_fast_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHY7cpdhCo9g"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_fast_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_fast)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_fast_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSskRoPqCo6N"
      },
      "source": [
        "'''\n",
        "del H_left_fast, H_right_fast,keypoints_all_left_fast, keypoints_all_right_fast, descriptors_all_left_fast, descriptors_all_right_fast, points_all_left_fast, points_all_right_fast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFQQsAnmCo2h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPqE-tN3Coy1"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_orb_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_orb = []\n",
        "descriptors_all_left_orb = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_orb.append(keypoints_each)\n",
        "  descriptors_all_left_orb.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDINZCa2Coth"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_orb_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_orb = []\n",
        "descriptors_all_right_orb = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_orb.append(keypoints_each)\n",
        "  descriptors_all_right_orb.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WHgugSBCony"
      },
      "source": [
        "'''\n",
        "H_left_orb = []\n",
        "H_right_orb = []\n",
        "\n",
        "num_matches_orb = []\n",
        "num_good_matches_orb = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_orb[j:j+2][::-1],points_all_left_orb[j:j+2][::-1],descriptors_all_left_orb[j:j+2][::-1],0.7)\n",
        "  H_left_orb.append(H_a)\n",
        "  num_matches_orb.append(matches)\n",
        "  num_good_matches_orb.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_orb[j:j+2][::-1],points_all_right_orb[j:j+2][::-1],descriptors_all_right_orb[j:j+2][::-1],0.7)\n",
        "  H_right_orb.append(H_a)\n",
        "  num_matches_orb.append(matches)\n",
        "  num_good_matches_orb.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KOqm_thCoiE"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_orb_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_orb)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_orb_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Thqr2JOCoRY"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_orb_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_orb)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_orb_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od2bhrUTCoMK"
      },
      "source": [
        "'''\n",
        "del H_left_orb, H_right_orb,keypoints_all_left_orb, keypoints_all_right_orb, descriptors_all_left_orb, descriptors_all_right_orb, points_all_left_orb, points_all_right_orb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9jqLTS8CoF6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8JpQMQ8Cn_a"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_kaze_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_kaze = []\n",
        "descriptors_all_left_kaze = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_kaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_kaze.append(keypoints_each)\n",
        "  descriptors_all_left_kaze.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIxJL3eVLU4-"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_kaze_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_kaze = []\n",
        "descriptors_all_right_kaze = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_kaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_kaze.append(keypoints_each)\n",
        "  descriptors_all_right_kaze.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVmEjVvtLU1c"
      },
      "source": [
        "'''\n",
        "H_left_kaze = []\n",
        "H_right_kaze = []\n",
        "\n",
        "num_matches_kaze = []\n",
        "num_good_matches_kaze = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_kaze[j:j+2][::-1],points_all_left_kaze[j:j+2][::-1],descriptors_all_left_kaze[j:j+2][::-1])\n",
        "  H_left_kaze.append(H_a)\n",
        "  num_matches_kaze.append(matches)\n",
        "  num_good_matches_kaze.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_kaze[j:j+2][::-1],points_all_right_kaze[j:j+2][::-1],descriptors_all_right_kaze[j:j+2][::-1])\n",
        "  H_right_kaze.append(H_a)\n",
        "  num_matches_kaze.append(matches)\n",
        "  num_good_matches_kaze.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guD4TGu6LUx3"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_kaze_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_kaze)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_kaze_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtlBrHx-LUsA"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_kaze_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_kaze)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_kaze_40.h5')/1.e6,'MB')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi9MYWV1LUog"
      },
      "source": [
        "'''\n",
        "del H_left_kaze, H_right_kaze,keypoints_all_left_kaze, keypoints_all_right_kaze, descriptors_all_left_kaze, descriptors_all_right_kaze, points_all_left_kaze, points_all_right_kaze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na358EdCLUlI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yygpK4ReLUgv"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_akaze_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_akaze = []\n",
        "descriptors_all_left_akaze = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_akaze.append(keypoints_each)\n",
        "  descriptors_all_left_akaze.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMVyOCZkLUa8"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_akaze_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_akaze = []\n",
        "descriptors_all_right_akaze = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_akaze.append(keypoints_each)\n",
        "  descriptors_all_right_akaze.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXXoYuWrLUVi"
      },
      "source": [
        "'''\n",
        "H_left_akaze = []\n",
        "H_right_akaze = []\n",
        "\n",
        "num_matches_akaze = []\n",
        "num_good_matches_akaze = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_akaze[j:j+2][::-1],points_all_left_akaze[j:j+2][::-1],descriptors_all_left_akaze[j:j+2][::-1])\n",
        "  H_left_akaze.append(H_a)\n",
        "  num_matches_akaze.append(matches)\n",
        "  num_good_matches_akaze.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_akaze[j:j+2][::-1],points_all_right_akaze[j:j+2][::-1],descriptors_all_right_akaze[j:j+2][::-1])\n",
        "  H_right_akaze.append(H_a)\n",
        "  num_matches_akaze.append(matches)\n",
        "  num_good_matches_akaze.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUbYsdqLUPW"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_akaze_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_akaze)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_akaze_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKB4xG_yLrCm"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_akaze_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_akaze)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_akaze_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTsWFn9ULq-9"
      },
      "source": [
        "'''\n",
        "del H_left_akaze, H_right_akaze,keypoints_all_left_akaze, keypoints_all_right_akaze, descriptors_all_left_akaze, descriptors_all_right_akaze, points_all_left_akaze, points_all_right_akaze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEmrP5gQLq7D"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_star_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_star = []\n",
        "descriptors_all_left_brief = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_star.append(keypoints_each)\n",
        "  descriptors_all_left_brief.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a37U2l41Lq3d"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_star_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_star = []\n",
        "descriptors_all_right_brief = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_star.append(keypoints_each)\n",
        "  descriptors_all_right_brief.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AqNG-R1Lqzm"
      },
      "source": [
        "'''\n",
        "H_left_brief = []\n",
        "H_right_brief = []\n",
        "\n",
        "num_matches_briefstar = []\n",
        "num_good_matches_briefstar = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_star[j:j+2][::-1],points_all_left_star[j:j+2][::-1],descriptors_all_left_brief[j:j+2][::-1])\n",
        "  H_left_brief.append(H_a)\n",
        "  num_matches_briefstar.append(matches)\n",
        "  num_good_matches_briefstar.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_star[j:j+2][::-1],points_all_right_star[j:j+2][::-1],descriptors_all_right_brief[j:j+2][::-1])\n",
        "  H_right_brief.append(H_a)\n",
        "  num_matches_briefstar.append(matches)\n",
        "  num_good_matches_briefstar.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0rUJtNXL_3A"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_brief_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_brief)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_brief_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UcMra4TL_ya"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_brief_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_brief)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_brief_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2byBTnoL_t_"
      },
      "source": [
        "'''\n",
        "del H_left_brief, H_right_brief,keypoints_all_left_star, keypoints_all_right_star, descriptors_all_left_brief, descriptors_all_right_brief, points_all_left_star, points_all_right_star"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbdGzY8tL_m0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMQUSHH2L_hd"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_agast_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_agast = []\n",
        "descriptors_all_left_agast = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_agast.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_agast.append(keypoints_each)\n",
        "  descriptors_all_left_agast.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8vgi650L_dN"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_agast_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_agast = []\n",
        "descriptors_all_right_agast = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_agast.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_agast.append(keypoints_each)\n",
        "  descriptors_all_right_agast.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06H-GZvbL_X7"
      },
      "source": [
        "'''\n",
        "H_left_agast = []\n",
        "H_right_agast = []\n",
        "\n",
        "num_matches_agast = []\n",
        "num_good_matches_agast = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_agast[j:j+2][::-1],points_all_left_agast[j:j+2][::-1],descriptors_all_left_agast[j:j+2][::-1],0.85,6)\n",
        "  H_left_agast.append(H_a)\n",
        "  num_matches_agast.append(matches)\n",
        "  num_good_matches_agast.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_agast[j:j+2][::-1],points_all_right_agast[j:j+2][::-1],descriptors_all_right_agast[j:j+2][::-1],0.85,6)\n",
        "  H_right_agast.append(H_a)\n",
        "  num_matches_agast.append(matches)\n",
        "  num_good_matches_agast.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuB1EagSL_Ta"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_agast_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_agast)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_agast_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LMoHOzL_NW"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_agast_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_agast)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_agast_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kYn3bzmMPyl"
      },
      "source": [
        "'''\n",
        "del H_left_agast, H_right_agast,keypoints_all_left_agast, keypoints_all_right_agast, descriptors_all_left_agast, descriptors_all_right_agast, points_all_left_agast, points_all_right_agast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZbR8ZLbMPuK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1wEtkqiMPo4"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_daisy_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_daisy = []\n",
        "descriptors_all_left_daisy = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_daisy.append(keypoints_each)\n",
        "  descriptors_all_left_daisy.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weAFvO6GMPjf"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_daisy_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_daisy = []\n",
        "descriptors_all_right_daisy = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_daisy.append(keypoints_each)\n",
        "  descriptors_all_right_daisy.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2rNYL0GMPd4"
      },
      "source": [
        "'''\n",
        "H_left_daisy = []\n",
        "H_right_daisy = []\n",
        "\n",
        "num_matches_daisy = []\n",
        "num_good_matches_daisy = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_daisy[j:j+2][::-1],points_all_left_daisy[j:j+2][::-1],descriptors_all_left_daisy[j:j+2][::-1],0.7,6)\n",
        "  H_left_daisy.append(H_a)\n",
        "  num_matches_daisy.append(matches)\n",
        "  num_good_matches_daisy.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_daisy[j:j+2][::-1],points_all_right_daisy[j:j+2][::-1],descriptors_all_right_daisy[j:j+2][::-1],0.7,6)\n",
        "  H_right_daisy.append(H_a)\n",
        "  num_matches_daisy.append(matches)\n",
        "  num_good_matches_daisy.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbmBqPPoMPXn"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_daisy_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_daisy)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_daisy_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7kN0xeSMcJD"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_daisy_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_daisy)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_daisy_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJz1GnPlMcFv"
      },
      "source": [
        "'''\n",
        "del H_left_daisy, H_right_daisy,keypoints_all_left_daisy, keypoints_all_right_daisy, descriptors_all_left_daisy, descriptors_all_right_daisy, points_all_left_daisy, points_all_right_daisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBhvFt7CMcAe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1uKem3vMb7M"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_freak_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_freak = []\n",
        "descriptors_all_left_freak = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_freak.append(keypoints_each)\n",
        "  descriptors_all_left_freak.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCTwXuf6Mb0W"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_freak_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_freak = []\n",
        "descriptors_all_right_freak = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_freak.append(keypoints_each)\n",
        "  descriptors_all_right_freak.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nojIEqNvMm0b"
      },
      "source": [
        "'''\n",
        "H_left_freak = []\n",
        "H_right_freak = []\n",
        "\n",
        "num_matches_freak = []\n",
        "num_good_matches_freak = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_freak[j:j+2][::-1],points_all_left_freak[j:j+2][::-1],descriptors_all_left_freak[j:j+2][::-1],0.7,6)\n",
        "  H_left_freak.append(H_a)\n",
        "  num_matches_freak.append(matches)\n",
        "  num_good_matches_freak.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_freak[j:j+2][::-1],points_all_right_freak[j:j+2][::-1],descriptors_all_right_freak[j:j+2][::-1],0.7,6)\n",
        "  H_right_freak.append(H_a)\n",
        "  num_matches_freak.append(matches)\n",
        "  num_good_matches_freak.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWtrnneyMmwE"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_freak_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_freak)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_freak_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGRvI_mjMmqY"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_freak_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_freak)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_freak_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM9RneyGMml4"
      },
      "source": [
        "'''\n",
        "del H_left_freak, H_right_freak,keypoints_all_left_freak, keypoints_all_right_freak, descriptors_all_left_freak, descriptors_all_right_freak, points_all_left_freak, points_all_right_freak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XtCEAI_MmhW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSA_55TFMmaa"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_surf_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_surf = []\n",
        "descriptors_all_left_surf = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_surf.append(keypoints_each)\n",
        "  descriptors_all_left_surf.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPc9Ec71M1bH"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_surf_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_surf = []\n",
        "descriptors_all_right_surf = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_surf.append(keypoints_each)\n",
        "  descriptors_all_right_surf.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL8Xcg9kM1V6"
      },
      "source": [
        "'''\n",
        "H_left_surf = []\n",
        "H_right_surf = []\n",
        "\n",
        "num_matches_surf = []\n",
        "num_good_matches_surf = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_surf[j:j+2][::-1],points_all_left_surf[j:j+2][::-1],descriptors_all_left_surf[j:j+2][::-1],0.65)\n",
        "  H_left_surf.append(H_a)\n",
        "  num_matches_surf.append(matches)\n",
        "  num_good_matches_surf.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_surf[j:j+2][::-1],points_all_right_surf[j:j+2][::-1],descriptors_all_right_surf[j:j+2][::-1],0.65)\n",
        "  H_right_surf.append(H_a)\n",
        "  num_matches_surf.append(matches)\n",
        "  num_good_matches_surf.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmxdDHHtM1RP"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_surf_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_surf)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_surf_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHnkqEQaM1GG"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_surf_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_surf)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_surf_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWnqIKCGM1CZ"
      },
      "source": [
        "'''\n",
        "del H_left_surf, H_right_surf,keypoints_all_left_surf, keypoints_all_right_surf, descriptors_all_left_surf, descriptors_all_right_surf, points_all_left_surf, points_all_right_surf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRiNLo19M0-z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnE1eMQFM07J"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_rootsift = []\n",
        "descriptors_all_left_rootsift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_rootsift.append(keypoints_each)\n",
        "  descriptors_all_left_rootsift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho55GFaxM02H"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_rootsift = []\n",
        "descriptors_all_right_rootsift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_rootsift.append(keypoints_each)\n",
        "  descriptors_all_right_rootsift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFOkZhTNNBra"
      },
      "source": [
        "'''\n",
        "H_left_rootsift = []\n",
        "H_right_rootsift = []\n",
        "\n",
        "num_matches_rootsift = []\n",
        "num_good_matches_rootsift = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_rootsift[j:j+2][::-1],points_all_left_rootsift[j:j+2][::-1],descriptors_all_left_rootsift[j:j+2][::-1],0.9)\n",
        "  H_left_rootsift.append(H_a)\n",
        "  num_matches_rootsift.append(matches)\n",
        "  num_good_matches_rootsift.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_rootsift[j:j+2][::-1],points_all_right_rootsift[j:j+2][::-1],descriptors_all_right_rootsift[j:j+2][::-1],0.9)\n",
        "  H_right_rootsift.append(H_a)\n",
        "  num_matches_rootsift.append(matches)\n",
        "  num_good_matches_rootsift.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Rs4WF_NBn3"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_rootsift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_rootsift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_rootsift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNe5ZQs0NBj6"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_rootsift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_rootsift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_rootsift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-HuMiSpNBez"
      },
      "source": [
        "'''\n",
        "del H_left_rootsift, H_right_rootsift,keypoints_all_left_rootsift, keypoints_all_right_rootsift, descriptors_all_left_rootsift, descriptors_all_right_rootsift, points_all_left_rootsift, points_all_right_rootsift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvOdNUKWNBa2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox8i17_-Tslv"
      },
      "source": [
        "\n",
        "import pickle\n",
        "Fdb = open('all_feat_surfsift_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_surfsift = []\n",
        "descriptors_all_left_surfsift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_surfsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_surfsift.append(keypoints_each)\n",
        "  descriptors_all_left_surfsift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_rHNxPCVcj9"
      },
      "source": [
        "\n",
        "import pickle\n",
        "Fdb = open('all_feat_surfsift_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_surfsift = []\n",
        "descriptors_all_right_surfsift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_surfsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_surfsift.append(keypoints_each)\n",
        "  descriptors_all_right_surfsift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FImXVRt7Vcfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59dd55ba-5d61-4244-da9f-6510fcff9ce6"
      },
      "source": [
        "\n",
        "H_left_surfsift = []\n",
        "H_right_surfsift = []\n",
        "\n",
        "num_matches_surfsift = []\n",
        "num_good_matches_surfsift = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_surfsift[j:j+2][::-1],points_all_left_surfsift[j:j+2][::-1],descriptors_all_left_surfsift[j:j+2][::-1],0.7,6)\n",
        "  H_left_surfsift.append(H_a)\n",
        "  num_matches_surfsift.append(matches)\n",
        "  num_good_matches_surfsift.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_surfsift[j:j+2][::-1],points_all_right_surfsift[j:j+2][::-1],descriptors_all_right_surfsift[j:j+2][::-1],0.7,6)\n",
        "  H_right_surfsift.append(H_a)\n",
        "  num_matches_surfsift.append(matches)\n",
        "  num_good_matches_surfsift.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/31 [00:13<06:34, 13.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 54132\n",
            "Number of matches After Lowe's Ratio 11061\n",
            "Number of Robust matches 9699\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 57372\n",
            "Number of matches After Lowe's Ratio 11630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 2/31 [00:26<06:22, 13.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 10298\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 3/31 [00:40<06:15, 13.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 57411\n",
            "Number of matches After Lowe's Ratio 12438\n",
            "Number of Robust matches 8723\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/31 [00:54<06:09, 13.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59112\n",
            "Number of matches After Lowe's Ratio 12030\n",
            "Number of Robust matches 10601\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 5/31 [01:08<06:00, 13.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 60791\n",
            "Number of matches After Lowe's Ratio 12361\n",
            "Number of Robust matches 10352\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 6/31 [01:22<05:49, 13.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 61194\n",
            "Number of matches After Lowe's Ratio 10931\n",
            "Number of Robust matches 9356\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/31 [01:37<05:40, 14.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 62210\n",
            "Number of matches After Lowe's Ratio 11136\n",
            "Number of Robust matches 9603\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 8/31 [01:51<05:31, 14.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 64509\n",
            "Number of matches After Lowe's Ratio 10680\n",
            "Number of Robust matches 9067\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 64280\n",
            "Number of matches After Lowe's Ratio 9695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 9/31 [02:06<05:19, 14.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 8248\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 69893\n",
            "Number of matches After Lowe's Ratio 10068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 10/31 [02:22<05:12, 14.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 7843\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 11/31 [02:38<05:06, 15.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 66826\n",
            "Number of matches After Lowe's Ratio 296\n",
            "Number of Robust matches 208\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▊      | 12/31 [02:56<05:04, 16.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 63647\n",
            "Number of matches After Lowe's Ratio 24076\n",
            "Number of Robust matches 21487\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 13/31 [03:11<04:42, 15.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 62298\n",
            "Number of matches After Lowe's Ratio 12453\n",
            "Number of Robust matches 10692\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 14/31 [03:26<04:22, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 61477\n",
            "Number of matches After Lowe's Ratio 13066\n",
            "Number of Robust matches 11182\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 15/31 [03:41<04:04, 15.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59923\n",
            "Number of matches After Lowe's Ratio 13946\n",
            "Number of Robust matches 12114\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 16/31 [04:00<04:06, 16.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59836\n",
            "Number of matches After Lowe's Ratio 14664\n",
            "Number of Robust matches 12440\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 17/31 [04:14<03:41, 15.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 60180\n",
            "Number of matches After Lowe's Ratio 14466\n",
            "Number of Robust matches 12480\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 18/31 [04:29<03:20, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 61057\n",
            "Number of matches After Lowe's Ratio 13924\n",
            "Number of Robust matches 11933\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 59667\n",
            "Number of matches After Lowe's Ratio 12071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████▏   | 19/31 [04:46<03:12, 16.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 8734\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 20/31 [05:04<03:01, 16.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59078\n",
            "Number of matches After Lowe's Ratio 12269\n",
            "Number of Robust matches 10052\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 21/31 [05:22<02:49, 16.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 63337\n",
            "Number of matches After Lowe's Ratio 10363\n",
            "Number of Robust matches 7525\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 22/31 [05:40<02:36, 17.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 60232\n",
            "Number of matches After Lowe's Ratio 1207\n",
            "Number of Robust matches 748\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 60343\n",
            "Number of matches After Lowe's Ratio 29101\n",
            "Number of Robust matches 26162\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 24/31 [06:12<01:55, 16.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 55994\n",
            "Number of matches After Lowe's Ratio 14538\n",
            "Number of Robust matches 12761\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 57703\n",
            "Number of matches After Lowe's Ratio 13122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 25/31 [06:26<01:33, 15.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 10399\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 26/31 [06:39<01:14, 14.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 58128\n",
            "Number of matches After Lowe's Ratio 14189\n",
            "Number of Robust matches 10172\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 27/31 [06:53<00:58, 14.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59949\n",
            "Number of matches After Lowe's Ratio 14264\n",
            "Number of Robust matches 11455\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 28/31 [07:12<00:47, 15.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 63457\n",
            "Number of matches After Lowe's Ratio 12713\n",
            "Number of Robust matches 10409\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▎| 29/31 [07:27<00:31, 15.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59685\n",
            "Number of matches After Lowe's Ratio 2389\n",
            "Number of Robust matches 1883\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 30/31 [07:41<00:15, 15.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 58605\n",
            "Number of matches After Lowe's Ratio 19690\n",
            "Number of Robust matches 17163\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/30 [00:12<06:02, 12.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 54464\n",
            "Number of matches After Lowe's Ratio 11571\n",
            "Number of Robust matches 10097\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 2/30 [00:25<05:54, 12.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 52975\n",
            "Number of matches After Lowe's Ratio 10391\n",
            "Number of Robust matches 9033\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [00:38<05:54, 13.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 52701\n",
            "Number of matches After Lowe's Ratio 9103\n",
            "Number of Robust matches 7330\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/30 [00:52<05:48, 13.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 55340\n",
            "Number of matches After Lowe's Ratio 11492\n",
            "Number of Robust matches 9700\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 5/30 [01:06<05:40, 13.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 55219\n",
            "Number of matches After Lowe's Ratio 26075\n",
            "Number of Robust matches 22716\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 6/30 [01:20<05:23, 13.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 52863\n",
            "Number of matches After Lowe's Ratio 381\n",
            "Number of Robust matches 283\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 50244\n",
            "Number of matches After Lowe's Ratio 7919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/30 [01:32<05:02, 13.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 6691\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 8/30 [01:44<04:41, 12.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 52003\n",
            "Number of matches After Lowe's Ratio 9288\n",
            "Number of Robust matches 7173\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 9/30 [01:57<04:27, 12.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 51993\n",
            "Number of matches After Lowe's Ratio 9917\n",
            "Number of Robust matches 8115\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 10/30 [02:09<04:13, 12.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 52354\n",
            "Number of matches After Lowe's Ratio 10033\n",
            "Number of Robust matches 7491\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 52060\n",
            "Number of matches After Lowe's Ratio 9567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 11/30 [02:22<04:01, 12.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 7962\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 12/30 [02:34<03:45, 12.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 53679\n",
            "Number of matches After Lowe's Ratio 9970\n",
            "Number of Robust matches 8708\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 13/30 [02:47<03:35, 12.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 53905\n",
            "Number of matches After Lowe's Ratio 10432\n",
            "Number of Robust matches 9178\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 14/30 [03:01<03:26, 12.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 55662\n",
            "Number of matches After Lowe's Ratio 10093\n",
            "Number of Robust matches 8849\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 57274\n",
            "Number of matches After Lowe's Ratio 10416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 15/30 [03:14<03:16, 13.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 9171\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 16/30 [03:28<03:07, 13.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 59389\n",
            "Number of matches After Lowe's Ratio 10730\n",
            "Number of Robust matches 8622\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 17/30 [03:42<02:57, 13.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 61036\n",
            "Number of matches After Lowe's Ratio 10262\n",
            "Number of Robust matches 8952\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 18/30 [04:03<03:07, 15.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 63331\n",
            "Number of matches After Lowe's Ratio 9555\n",
            "Number of Robust matches 7311\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 19/30 [04:23<03:06, 16.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 63047\n",
            "Number of matches After Lowe's Ratio 9206\n",
            "Number of Robust matches 7695\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 20/30 [04:43<02:59, 17.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 64427\n",
            "Number of matches After Lowe's Ratio 10062\n",
            "Number of Robust matches 8107\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 21/30 [05:04<02:49, 18.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 64463\n",
            "Number of matches After Lowe's Ratio 9430\n",
            "Number of Robust matches 7236\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 66428\n",
            "Number of matches After Lowe's Ratio 10136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 22/30 [05:25<02:36, 19.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 7500\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 68886\n",
            "Number of matches After Lowe's Ratio 10954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 23/30 [05:47<02:21, 20.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 8731\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 24/30 [06:09<02:05, 20.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 68541\n",
            "Number of matches After Lowe's Ratio 10497\n",
            "Number of Robust matches 8700\n",
            "\n",
            "\n",
            "\n",
            "Number of matches 73840\n",
            "Number of matches After Lowe's Ratio 21789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 25/30 [06:31<01:46, 21.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Robust matches 15368\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 26/30 [06:49<01:20, 20.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 66432\n",
            "Number of matches After Lowe's Ratio 788\n",
            "Number of Robust matches 559\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 27/30 [07:05<00:56, 18.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 66818\n",
            "Number of matches After Lowe's Ratio 8831\n",
            "Number of Robust matches 5972\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 28/30 [07:21<00:36, 18.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 67520\n",
            "Number of matches After Lowe's Ratio 9721\n",
            "Number of Robust matches 7609\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 29/30 [07:37<00:15, 15.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of matches 67553\n",
            "Number of matches After Lowe's Ratio 9382\n",
            "Number of Robust matches 5803\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xMUiQctVcac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e73728-22e1-4e0f-a85c-8f7084bd4ca6"
      },
      "source": [
        "\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_surfsift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_surfsift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_surfsift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HDF5  w/o comp.: 0.010499000549316406 [s] ... size 0.004208 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCs2pUGAVcTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64daf9b9-6eae-4702-84c1-71c99c0ada14"
      },
      "source": [
        "\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_surfsift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_surfsift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_surfsift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HDF5  w/o comp.: 0.005301475524902344 [s] ... size 0.004136 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQyjlNKdVkm3"
      },
      "source": [
        "\n",
        "del H_left_surfsift, H_right_surfsift,keypoints_all_left_surfsift, keypoints_all_right_surfsift, descriptors_all_left_surfsift, descriptors_all_right_surfsift, points_all_left_surfsift, points_all_right_surfsift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDZPXKE7Vkjm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyC-HQL6NSGE"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_gftt_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_gftt = []\n",
        "descriptors_all_left_gftt = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_gftt.append(keypoints_each)\n",
        "  descriptors_all_left_gftt.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQSn8pA5NSBw"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_gftt_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_gftt = []\n",
        "descriptors_all_right_gftt = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_gftt.append(keypoints_each)\n",
        "  descriptors_all_right_gftt.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDiMSQY4NR-l"
      },
      "source": [
        "'''\n",
        "H_left_gftt = []\n",
        "H_right_gftt = []\n",
        "\n",
        "num_matches_gftt = []\n",
        "num_good_matches_gftt = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_gftt[j:j+2][::-1],points_all_left_gftt[j:j+2][::-1],descriptors_all_left_gftt[j:j+2][::-1],0.85,6)\n",
        "  H_left_gftt.append(H_a)\n",
        "  num_matches_gftt.append(matches)\n",
        "  num_good_matches_gftt.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_gftt[j:j+2][::-1],points_all_right_gftt[j:j+2][::-1],descriptors_all_right_gftt[j:j+2][::-1],0.85,6)\n",
        "  H_right_gftt.append(H_a)\n",
        "  num_matches_gftt.append(matches)\n",
        "  num_good_matches_gftt.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNfb9ajaNR51"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_gftt_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_gftt)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_gftt_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmDMHiUVNR0n"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_gftt_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_gftt)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_gftt_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjou3NzdNRvb"
      },
      "source": [
        "'''\n",
        "del H_left_gftt, H_right_gftt,keypoints_all_left_gftt, keypoints_all_right_gftt, descriptors_all_left_gftt, descriptors_all_right_gftt, points_all_left_gftt, points_all_right_gftt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WPAx1bPNRpb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkzmrEaNNRld"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_mser_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_mser = []\n",
        "descriptors_all_left_mser = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_mser.append(keypoints_each)\n",
        "  descriptors_all_left_mser.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJrHMRuwNRgV"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_mser_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_mser = []\n",
        "descriptors_all_right_mser = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_mser.append(keypoints_each)\n",
        "  descriptors_all_right_mser.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTNrAvibNRWJ"
      },
      "source": [
        "'''\n",
        "H_left_mser = []\n",
        "H_right_mser = []\n",
        "\n",
        "num_matches_mser = []\n",
        "num_good_matches_mser = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_mser[j:j+2][::-1],points_all_left_mser[j:j+2][::-1],descriptors_all_left_mser[j:j+2][::-1],0.95,8)\n",
        "  H_left_mser.append(H_a)\n",
        "  num_matches_mser.append(matches)\n",
        "  num_good_matches_mser.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_mser[j:j+2][::-1],points_all_right_mser[j:j+2][::-1],descriptors_all_right_mser[j:j+2][::-1],0.95,8)\n",
        "  H_right_mser.append(H_a)\n",
        "  num_matches_mser.append(matches)\n",
        "  num_good_matches_mser.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15_8796hNjIP"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_mser_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_mser)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_mser_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Er9whfNjE0"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_mser_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_mser)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_mser_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGinOcpTNjA9"
      },
      "source": [
        "'''\n",
        "del H_left_mser, H_right_mser,keypoints_all_left_mser, keypoints_all_right_mser, descriptors_all_left_mser, descriptors_all_right_mser, points_all_left_mser, points_all_right_mser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60PMVL7-Ni9a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9h1dfs3Ni5A"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_superpoint_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_superpoint = []\n",
        "descriptors_all_left_superpoint = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_superpoint.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_superpoint.append(keypoints_each)\n",
        "  descriptors_all_left_superpoint.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88AByrvBNi0y"
      },
      "source": [
        "'''\n",
        "import pickle\n",
        "Fdb = open('all_feat_superpoint_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_superpoint = []\n",
        "descriptors_all_right_superpoint = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_superpoint.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_superpoint.append(keypoints_each)\n",
        "  descriptors_all_right_superpoint.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2kldmCnNiwl"
      },
      "source": [
        "'''\n",
        "H_left_superpoint = []\n",
        "H_right_superpoint = []\n",
        "\n",
        "num_matches_superpoint = []\n",
        "num_good_matches_superpoint = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_superpoint[j:j+2][::-1],points_all_left_superpoint[j:j+2][::-1],descriptors_all_left_superpoint[j:j+2][::-1],ratio=0.8,thresh=3,no_ransac=False,use_lowe=True)\n",
        "  H_left_superpoint.append(H_a)\n",
        "  num_matches_superpoint.append(matches)\n",
        "  num_good_matches_superpoint.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_superpoint[j:j+2][::-1],points_all_right_superpoint[j:j+2][::-1],descriptors_all_right_superpoint[j:j+2][::-1],ratio=0.8,thresh = 3,no_ransac=False,use_lowe=True)\n",
        "  H_right_superpoint.append(H_a)\n",
        "  num_matches_superpoint.append(matches)\n",
        "  num_good_matches_superpoint.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q706DNjLN6Vh"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_superpoint_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_superpoint)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_superpoint_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkzXTRmkN6RW"
      },
      "source": [
        "'''\n",
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_superpoint_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_superpoint)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_superpoint_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFL3LP99N6MH"
      },
      "source": [
        "'''\n",
        "del H_left_superpoint, H_right_superpoint,keypoints_all_left_superpoint, keypoints_all_right_superpoint, descriptors_all_left_superpoint, descriptors_all_right_superpoint, points_all_left_superpoint, points_all_right_superpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q_yrn5XOFoP"
      },
      "source": [
        "'''\n",
        "print(len(num_matches_superpoint))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iel-_BchOFjx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HlL5luZaEOc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trzf2eTGOOjd"
      },
      "source": [
        "Collect All Number Of KeyPoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HS4lRFrVkfT"
      },
      "source": [
        "len_files = len(left_files_path) + len(right_files_path[1:])\n",
        "num_detectors = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCck1fMMVkZ1"
      },
      "source": [
        "d = {'Dataset': [f'{Dataset}']*(num_detectors*len_files), 'Number of Keypoints': num_kps_surfsift, 'Detector/Descriptor':  ['SURF+SIFT']*len_files  }\n",
        "df_numkey_1 = pd.DataFrame(data=d)\n",
        "df_numkey_1['Number of Keypoints'] = df_numkey_1['Number of Keypoints']/(len_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAg-a2j-OVqA"
      },
      "source": [
        "#d = {'Dataset': ['University Campus']*(3*len_files), 'Number of Keypoints': num_kps_rootsift + num_kps_superpoint + num_kps_surf, 'Detector/Descriptor':['ROOTSIFT']*101 + ['SuperPoint']*101 + ['SURF']*101  }\n",
        "#df = pd.DataFrame(data=d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2PQojiuOVk0"
      },
      "source": [
        "#df_13 = pd.read_csv('drive/MyDrive/Num_Key_13.csv')\n",
        "#frames = [df_13, df]\n",
        "#df_16 = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3OVBuQNOVgo"
      },
      "source": [
        "#df_16.to_csv('drive/MyDrive/Num_Key_16.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRowM7PgOVZN"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_numkey_1, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Number of Keypoints\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=6, aspect=2\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Number of Keypoints/Descriptors\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Number of Keypoints Detected for each Detector/Descriptor in Different Aerial Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr91y79XOVR8"
      },
      "source": [
        "g.savefig(f'drive/MyDrive/Num_Kypoints_7_{Dataset}.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDRlBi3XOd8f"
      },
      "source": [
        "df_numkey_1.to_csv(f'drive/MyDrive/Num_Kypoints_1_{Dataset}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJa1lqz8oLgi"
      },
      "source": [
        "#print(len(num_matches_agast))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8IMi-qOhTI"
      },
      "source": [
        "Total Number of Matches Detected for each Detector+Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmA0wyBIOd3l"
      },
      "source": [
        "#df_match_15['Number of Total Matches'] =  num_matches_agast + num_matches_akaze + num_matches_brisk + num_matches_daisy + num_matches_fast + num_matches_freak + num_matches_gftt + num_matches_kaze + num_matches_mser + num_matches_orb + num_matches_rootsift + num_matches_sift + num_matches_briefstar + num_matches_superpoint+ num_matches_surf+ num_matches_surfsift\n",
        "d = {'Dataset': [f'{Dataset}']*(num_detectors*(len_files-1)), 'Number of Total Matches': num_matches_surfsift , 'Detector/Descriptor':   ['SURF+SIFT']*(len_files-1)  }\n",
        "df_match_1 = pd.DataFrame(data=d)\n",
        "df_match_1['Number of Total Matches'] = df_match_1['Number of Total Matches']/(len_files-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUZ8fqlHOdzR"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_7, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Number of Total Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset \", \"Total Number of Matches b/w Consecutive/Overlapping Images\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Total Number of Matches Detected for each Detector/Descriptor in Different Aerial Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_WMsBjmOdu1"
      },
      "source": [
        "g.savefig(f'drive/MyDrive/Num_Matches_7_{Dataset}.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv58xIv0Odqj"
      },
      "source": [
        "#df_match_16.to_csv('drive/MyDrive/Num_Matches_16.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPEmrVP9OrDR"
      },
      "source": [
        "Total Number of Good/Robust Matches (NN+Lowe+RANSAC) Detected for each Detector+Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx1DHmzTOt3P"
      },
      "source": [
        "df_match_1['Number of Good Matches'] =  num_good_matches_surfsift\n",
        "df_match_1['Number of Good Matches'] = df_match_1['Number of Good Matches']/(len_files-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmL182-COtzw"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_7, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Number of Good Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Number of Good Matches b/w Consecutive/Overlapping Images\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Number of Good Matches (Lowe + RANSAC) Detected for each Detector/Descriptor in Different Aerial Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow42UY49OtvS"
      },
      "source": [
        "g.savefig('drive/MyDrive/Num_Good_Matches_7.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqbpFnFqOtro"
      },
      "source": [
        "#df_match_16.to_csv('drive/MyDrive/Num_Good_Matches_16.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgN-5TGVO1_A"
      },
      "source": [
        "Recall Rate for each Detector+Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq_VaaFpO2oh"
      },
      "source": [
        "df_match_1['Recall Rate of Matches'] = df_match_1['Number of Good Matches']/df_match_1['Number of Total Matches']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRdc-FrzO5Na"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "g = sns.catplot(\n",
        "    data=df_match_7, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Recall Rate of Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Precision of Matches\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Recall Rate of Matches Detected (Good/Total) for each Detector/Descriptor in Different Aerial Datasets (Higher the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkDLeij5O5IP"
      },
      "source": [
        "g.savefig('drive/MyDrive/Recall_Rate_Matches_7.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK0j7jsO-gP"
      },
      "source": [
        "1-Precision Rate for each Detector+Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbU_yfEfO5Cw"
      },
      "source": [
        "df_match_1['1 - Precision Rate of Matches'] = (df_match_1['Number of Total Matches'] - df_match_1['Number of Good Matches'])/df_match_1['Number of Total Matches']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQakh0q8O49P"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_7, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"1 - Precision Rate of Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset (120 Images)\", \"1 - Precision Rate of Matches\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"1 - Precision rate of Matches Detected (False/Total Matches) for each Detector/Descriptor in Different Aerial Datasets (Lower the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmcYnpiUO45J"
      },
      "source": [
        "g.savefig('drive/MyDrive/One_minus_Precision_Rate_Matches_7.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsiNPE0QPHQx"
      },
      "source": [
        "F-Score for each Detector+Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQdlNOnVPE77"
      },
      "source": [
        "df_match_1['F-Score'] = (2* (1 - df_match_1['1 - Precision Rate of Matches']) * df_match_1['Recall Rate of Matches'])/((1 - df_match_1['1 - Precision Rate of Matches']) + df_match_1['Recall Rate of Matches'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCtYcjcSPE4F"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_7, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"F-Score\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"F-Score\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"F-Score of Matches Detected (2*P*R/P+R) for each Detector/Descriptor in Different Aerial Datasets (Higher the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibvFlM-QPE0j"
      },
      "source": [
        "g.savefig('drive/MyDrive/F_Score_Rate_Matches_7.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8uLq4m2PEvl"
      },
      "source": [
        "df_match_1.to_csv('drive/MyDrive/All_metrics_1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAURAn7oPQrG"
      },
      "source": [
        "Time for each Detector+Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE704mWRPEqI"
      },
      "source": [
        "d = {'Dataset': [f'{Dataset}']*(num_detectors), 'Time': [time_all[0]], 'Detector/Descriptor':  ['SURF+SIFT']*(1)  }\n",
        "df_time_1 = pd.DataFrame(data=d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnBKna22PTV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e941b873-e81c-4ef7-8bcf-29955fd0a592"
      },
      "source": [
        "print(df_time_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Dataset         Time Detector/Descriptor\n",
            "0  MAP Dataset  2841.516914           SURF+SIFT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDc45piJPTQi"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_time_7, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Time\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Time (in sec)\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Time taken during Feature Extraction by each Detector/Descriptor in Different Aerial Datasets (Lower the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdWb9SaEPTM5"
      },
      "source": [
        "g.savefig('drive/MyDrive/Time_7.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpAp8atPPTJN"
      },
      "source": [
        "df_time_1.to_csv('drive/MyDrive/Time_1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYEIN3_QPca0"
      },
      "source": [
        "Stitching with CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5yhDnhjPTDI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}